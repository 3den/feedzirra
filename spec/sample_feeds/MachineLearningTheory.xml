<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	>

<channel>
	<title>Machine Learning (Theory)</title>
	<atom:link href="http://hunch.net/?feed=rss2" rel="self" type="application/rss+xml" />
	<link>http://hunch.net</link>
	<description>Machine learning and learning theory research</description>
	<pubDate>Wed, 21 Jan 2009 23:57:19 +0000</pubDate>
	<generator>http://wordpress.org/?v=2.6.2</generator>
	<language>en</language>
			<item>
		<title>Nearly all natural problems require nonlinearity</title>
		<link>http://hunch.net/?p=524</link>
		<comments>http://hunch.net/?p=524#comments</comments>
		<pubDate>Wed, 21 Jan 2009 23:57:19 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Machine Learning]]></category>

		<category><![CDATA[Supervised]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=524</guid>
		<description><![CDATA[One conventional wisdom is that learning algorithms with linear representations are sufficient to solve natural learning  problems.  This conventional wisdom appears unsupported by empirical evidence as far as I can tell.  In nearly all vision, language, robotics, and speech applications I know where machine learning is effectively applied, the approach involves either [...]]]></description>
			<content:encoded><![CDATA[<p>One conventional wisdom is that learning algorithms with linear representations are sufficient to solve natural learning  problems.  This conventional wisdom appears unsupported by empirical evidence as far as I can tell.  In nearly all vision, language, robotics, and speech applications I know where machine learning is effectively applied, the approach involves either a linear representation on hand crafted features capturing substantial nonlinearities or learning directly on nonlinear representations.  </p>
<p>There are a few exceptions to this&#8212;for example, if the problem of interest to you is predicting the next word given previous words, n-gram methods have been shown effective.  Viewed the right way, n-gram methods are essentially linear predictors on an enormous sparse feature space, learned from an enormous number of examples.  Hal&#8217;s post <a href="http://nlpers.blogspot.com/">here</a> describes some of this in more detail.</p>
<p>In contrast, if you go to a machine learning conference, a large number of the new algorithms are variations of learning on a linear representation.  This claim should be understood broadly to include (for example) kernel methods, random projection methods, and more traditionally linear representations such as the perceptron.  A basic question is: Why is the study of linear representations so prevalent?</p>
<p>There are several reasons for investigating the linear viewpoint.</p>
<ol>
<li>Linear learning is sufficient.  As discussed above, this is really only true in practice if you have sufficiently capable humans hand-engineering features.  On one hand, there is a compelling directness to that approach, but on the other it&#8217;s not the kind of approach which transfers well to new problems.</li>
<li>Linear learning is a compelling primitive.  Many of the effective approaches for nonlinear learning use some combination of linear primitives connected by nonlinearities to make a final prediction.  As such, there is a plausible hope that improvements in linear learning can be applied repeatedly in these more complex structures.</li>
<li>Linear learning is the only thing tractable, empirically.  This has a grain of truth to it, but it appears to be uncompelling when you get down to the nitty-gritty details.  On a dataset large enough to require efficient algorithms, you often want to use online learning.  And, when you use online learning with a pure linear representation, the limiting factor is the speed that data can be sucked into the CPU from the network or the disk.  If you aren&#8217;t doing something more interesting than plain vanilla linear prediction, you are wasting most of your CPU cycles.</li>
<li>Linear learning is the only thing tractable, theoretically.  There are certainly many statements and guarantees that we only know how to make with linear representations and (typically) convex losses.  However, there are fundamental limits to the extent that a well understood tool can be misused, and it&#8217;s important to understand that these theorems do not (and cannot) say that learning on a linear representation will solve some concrete problem like (say) face recognition from 10000 labeled examples.  In addition, there are some analysis methods which apply to nonlinear learning systems&#8212;my favorite example is <a href="http://hunch.net/?cat=12">learning reductions</a>, but there are others also.</li>
</ol>
<p>Some of the reasons for linear investigations appear sound, while others are simply variants of &#8220;looking where the light is&#8221;, which comes from an often retold story:<br />
At night you see someone searching the ground under a streetlight.  <br />
You ask, &#8220;What happened?&#8221; <br />
They say, &#8220;I&#8217;m looking for the keys I dropped in the bushes.&#8221;<br />
&#8220;But there aren&#8217;t any bushes where you are searching.&#8221; <br />
&#8220;Yes, but I can&#8217;t see over there.&#8221; </p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=524</wfw:commentRss>
		</item>
		<item>
		<title>Netflix prize within epsilon</title>
		<link>http://hunch.net/?p=527</link>
		<comments>http://hunch.net/?p=527#comments</comments>
		<pubDate>Mon, 19 Jan 2009 11:36:24 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Competitions]]></category>

		<category><![CDATA[Machine Learning]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=527</guid>
		<description><![CDATA[The competitors for the Netflix Prize are tantalizingly close winning the million dollar prize.  This year, BellKor and Commendo Research sent a combined solution that won the progress prize.  Reading the writeups 2 is instructive.  Several aspects of solutions are taken for granted including stochastic gradient descent, ensemble prediction, and targeting residuals [...]]]></description>
			<content:encoded><![CDATA[<p>The competitors for the <a href="http://www.netflixprize.com/leaderboard">Netflix Prize</a> are tantalizingly close winning the million dollar prize.  This year, <a href="http://www.research.att.com/~volinsky/netflix/">BellKor</a> and <a href="http://www.commendo.at/index.php?lang=0&#038;_0=2&#038;_1=3">Commendo Research</a> sent a combined solution that won the <a href="http://www.netflixprize.com/prize?id=5">progress prize</a>.  Reading the <a href="http://www.research.att.com/~volinsky/netflix/Bellkor2008.pdf">writeups</a> <a href="http://www.commendo.at/references/files/ProgressPrize2008_BigChaos.pdf">2</a> is instructive.  Several aspects of solutions are taken for granted including stochastic gradient descent, ensemble prediction, and targeting residuals (a form of boosting).  Relatively to last year, it appears that many approaches have added parameterizations, especially for the purpose of modeling through time.</p>
<p>The big question is: will they make the big prize?  At this point, the level of complexity in entering the competition is prohibitive, so perhaps only the existing competitors will continue to try.  (This equation might change drastically if the teams open source their existing solutions, including parameter settings.) One fear is that the progress is asymptoting on the wrong side of the 10% threshold.  In the first year, the teams progressed through 84.3% of the 10% gap, and in the second year, they progressed through just 64.4% of the remaining gap.  While these numbers suggest an asymptote on the wrong side, in the month since the progress prize another 34.0% improvement of the remainder has been achieved.  It&#8217;s remarkable that it&#8217;s too close to call, with just a 0.0035 RMSE gap to win the big prize.  Clever people finding just the right parameterization might very well succeed.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=527</wfw:commentRss>
		</item>
		<item>
		<title>Predictive Analytics World</title>
		<link>http://hunch.net/?p=516</link>
		<comments>http://hunch.net/?p=516#comments</comments>
		<pubDate>Fri, 09 Jan 2009 01:21:34 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Conferences]]></category>

		<category><![CDATA[Machine Learning]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=516</guid>
		<description><![CDATA[Carla Vicens and Eric Siegel contacted me about Predictive Analytics World in San Francisco February 18&#038;19, which I wasn&#8217;t familiar with.  A quick look at the agenda reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It&#8217;s interesting to see a business-focused machine learning conference, [...]]]></description>
			<content:encoded><![CDATA[<p>Carla Vicens and <a href="http://www.cs.columbia.edu/~evs/">Eric Siegel</a> contacted me about <a href="http://www.predictiveanalyticsworld.com/">Predictive Analytics World</a> in San Francisco February 18&#038;19, which I wasn&#8217;t familiar with.  A quick look at the <a href="http://www.predictiveanalyticsworld.com/agenda_overview.php">agenda</a> reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It&#8217;s interesting to see a business-focused machine learning conference, as it says that we are succeeding as a field.  If you are interested in deployed applications, you might attend.</p>
<p>Eric and I did a quick interview by email.</p>
<p>John &gt;<br />
I&#8217;ve mostly published and participated in academic machine learning conferences like ICML, COLT, and NIPS.   When I look at the <a href="http://www.predictiveanalyticsworld.com/agenda_overview.php">set of speakers and subjects</a> for your conference  I think &#8220;machine learning for business&#8221;.  Is that your understanding of things? What I&#8217;m trying to ask is: what do you view as the primary goal for this conference?</p>
<p>Eric &gt;<br />
<strong>You got it.  This is the business event focused on the commercial deployment of technology developed at the research conferences you named.  Academics&#8217; term, &#8220;machine learning,&#8221; is essentially synonymous with the business world&#8217;s &#8220;predictive modeling&#8221;.  Predictive Analytics World focuses on business applications of this technology, such as response modeling, churn modeling, email targeting, product recommendations, insurance pricing, and credit scoring.  PAW&#8217;s goal is to strengthen the business impact delivered by predictive analytics deployment, and establish new opportunities with predictive analytics.  The conference delivers case studies, expertise and resources to this end.</p>
<p>The conference program is designed to speak the language of <i>marketing and business professionals</i> using or planning to use predictive analytics to solve business challenges &#8212; but for the hands-on practitioner or analytical expert focused on <i>commercial deployment</i> who wishes to speak this same language, it&#8217;s an equally valuable event.</strong></p>
<p>John &gt;<br />
 People at academic conferences would hope that technology developed there can transfer into business use.  In your experience, does this happen?  And how fast or difficult is it?</p>
<p>Eric &gt;<br />
<strong>The best way to catalyze commercial deployment is to show the people it really works outside &#8220;the lab&#8221; - which is why PAW&#8217;s program is packed primarily with named case studies of commercial deployment.  These success stories answer your question with a resounding &#8220;yes&#8221; that the core technology developed academically is indeed put to use.</p>
<p>But predictive analytics has not yet been broadly adopted across all industries, although success stories show at least initial reach in each vertical.  So, sure, as one who previously wore a researcher&#8217;s hat, commercial deployment can feel slow; having solved the hardest theoretical, mathematical or statistical problems, the rest should go smoothly, right?  Not exactly.  The main challenges come in ramping up the business &#8220;consumer&#8221; on the technology so they see its value, positioning the technology in a way that provides business value, and, on the integration side, in preparing corporate data for predictive modeling (that&#8217;s a doozy!) and in integrating predict scores into existing systems and processes.  These things take time.</strong></p>
<p>John &gt;<br />
Sometimes people working in the academic world don&#8217;t have a good understanding of what the real problems are.  Do you have a sense of which areas of research are underemphasized in the academic world?</p>
<p>Eric &gt;<br />
<strong>To reach commercial success in deploying predictive analytics for the business applications I listed above, the main challenges are on the process and non-analytical integration side, rather than core machine learning technology; its good enough.  So, I don&#8217;t consider there to be glaring ommissions in the capabilities of core machine learning (I taught the machine learning graduate course at Columbia University and still consider Tom Mitchell&#8217;s textbook to be my bible). </p>
<p>On the other hand, there are always places where &#8220;real-world&#8221; data is going to bring researchers&#8217; attention to as-yet-unsolved problems.  A perfect example is the Netflix Prize, the current leader of which (and winner of the recent Progress Prize) will be speaking at PAW-09 - see <a href="http://www.predictiveanalyticsworld.com/agenda.php#advancedapproaches">here</a>.</strong></p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=516</wfw:commentRss>
		</item>
		<item>
		<title>Interesting Papers at SODA 2009</title>
		<link>http://hunch.net/?p=513</link>
		<comments>http://hunch.net/?p=513#comments</comments>
		<pubDate>Wed, 07 Jan 2009 16:35:04 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Conferences]]></category>

		<category><![CDATA[Papers]]></category>

		<category><![CDATA[Theory]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=513</guid>
		<description><![CDATA[Several talks seem potentially interesting to ML folks at this year&#8217;s SODA.

Maria-Florina Balcan, Avrim Blum, and Anupam Gupta, Approximate Clustering without the Approximation.  This paper gives reasonable algorithms with provable approximation guarantees for k-median and other notions of clustering.  It&#8217;s conceptually interesting, because it&#8217;s the second example I&#8217;ve seen where NP hardness is [...]]]></description>
			<content:encoded><![CDATA[<p>Several talks seem potentially interesting to ML folks at this year&#8217;s SODA.</p>
<ol>
<li><a href="http://www.cs.cmu.edu/~ninamf/">Maria-Florina Balcan</a>, <a href="http://www.cs.cmu.edu/~avrim/">Avrim Blum</a>, and <a href="http://www.cs.cmu.edu/~anupamg/">Anupam Gupta</a>, <a href="http://www.cs.cmu.edu/~ninamf/papers/bbg-clustering-soda.pdf">Approximate Clustering without the Approximation</a>.  This paper gives reasonable algorithms with provable approximation guarantees for k-median and other notions of clustering.  It&#8217;s conceptually interesting, because it&#8217;s the second example I&#8217;ve seen where NP hardness is subverted by changing the problem definition subtle but reasonable way.  Essentially, they show that if any near-approximation to an optimal solution is good, then it&#8217;s computationally easy to find a near-optimal solution.  This subtle shift bears serious thought.  A similar one occurred in <a href="http://hunch.net/~jl/projects/reductions/ranking.pdf">our ranking paper</a> with respect to minimum feedback arcset.  With two known examples, it suggests that many more NP-complete problems might be finessed into irrelevance in this style.</li>
<li><a href="http://yury.name/">Yury Lifshits</a> and <a href="http://www.cse.cuhk.edu.hk/~syzhang/">Shengyu Zhang</a>, <a href="http://yury.name/papers/combinatorial-algorithms.pdf">Combinatorial Algorithms for Nearest Neighbors, Near-Duplicates, and Small-World Design</a>.  The basic idea of this paper is that actually creating a metric with a valid triangle inequality inequality is hard for real-world problems, so it&#8217;s desirable to have a datastructure which works with a relaxed notion of triangle inequality.  The precise relaxation is more extreme than you might imagine, implying the associated algorithms give substantial potential speedups in incomparable applications.  Yuri tells me that a <a href="http://hunch.net/~jl/projects/cover_tree/icml_final/final-icml.pdf">cover tree</a> style &#8220;true O(n) space&#8221; algorithm is possible.  If worked out and implemented, I could imagine substantial use.</li>
<li><a href="http://www.cs.princeton.edu/~ehazan/">Elad Hazan</a> and <a href="http://www.cs.princeton.edu/~satyen/">Satyen Kale</a> <a href="http://www.cs.princeton.edu/~ehazan/papers/bandit-variance.pdf">Better Algorithms for Benign Bandits</a>. The basic idea of this paper is that in real-world applications, an adversary is less powerful than is commonly supposed, so carefully taking into account the observed variance can yield an algorithm which works much better in practice, without sacrificing the worst case performance.</li>
<li><a href="http://web.mit.edu/matulef/www/">Kevin Matulef</a>, <a href="http://www.cs.cmu.edu/~odonnell/">Ryan O&#8217;Donnell</a>, <a href="http://people.csail.mit.edu/ronitt/">Ronitt Rubinfeld</a>, <a href="http://www.cs.columbia.edu/~rocco/">Rocco Servedio</a>, <a href="http://www.cs.columbia.edu/~rocco/Public/soda09-halfspaces.pdf">Testing Halfspaces</a>. The basic point of this paper is that testing halfspaces is qualitatively easier than finding a good half space with respect to 0/1 loss.  Although the analysis is laughably far from practical, the result is striking, and it&#8217;s plausible that the algorithm works much better than the analysis.  The core algorithm is at least conceptually simple: test that two correlated random points have the same sign, with &#8220;yes&#8221; being evidence of a halfspace and &#8220;no&#8221; not.</li>
<li>I also particularly liked <a href="http://research.microsoft.com/en-us/um/people/peres/">Yuval Peres</a>&#8217;s invited talk <a href="http://www.siam.org/proceedings/soda/2009/SODA09_108_peresy.pdf">The Unreasonable Effectiveness of Martingales</a>.  Martingale&#8217;s are endemic to learning, especially online learning, and I suspect we can tighten and clarify several arguments using some of the techniques discussed.</li>
</ol>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=513</wfw:commentRss>
		</item>
		<item>
		<title>Adversarial Academia</title>
		<link>http://hunch.net/?p=499</link>
		<comments>http://hunch.net/?p=499#comments</comments>
		<pubDate>Sat, 27 Dec 2008 19:07:50 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Conferences]]></category>

		<category><![CDATA[Funding]]></category>

		<category><![CDATA[Machine Learning]]></category>

		<category><![CDATA[Research]]></category>

		<category><![CDATA[Reviewing
]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=499</guid>
		<description><![CDATA[One viewpoint on academia is that it is inherently adversarial: there are finite research dollars, positions, and students to work with, implying a zero-sum game between different participants.  This is not a viewpoint that I want to promote, as I consider it flawed.  However, I know several people believe strongly in this viewpoint, [...]]]></description>
			<content:encoded><![CDATA[<p>One viewpoint on academia is that it is inherently adversarial: there are finite research dollars, positions, and students to work with, implying a zero-sum game between different participants.  This is not a viewpoint that I want to promote, as I consider it flawed.  However, I know several people believe strongly in this viewpoint, and I have found it to have  substantial explanatory power.</p>
<p>For example:</p>
<ol>
<li>It explains why your paper was rejected based on poor logic.  The reviewer wasn&#8217;t concerned with research quality, but rather with rejecting a competitor.</li>
<li>It explains why professors rarely work together.  The goal of a non-tenured professor (at least) is to get tenure, and a case for tenure comes from a portfolio of work that is undisputably yours.</li>
<li>It explains why new research programs are not quickly adopted.  Adopting a competitor&#8217;s program is impossible, if your career is based on the competitor being wrong.</li>
</ol>
<p>Different academic groups subscribe to the adversarial viewpoint in different degrees.  In my experience, <a href="http://nips.cc/">NIPS</a> is the worst.  It is bad enough that the probability of a paper being accepted at NIPS is monotonically <i>decreasing</i> in it&#8217;s quality.  This is more than just my personal experience over a number of years, as it&#8217;s corroborated by others who have told me the same.  ICML (run by <a href="http://www.machinelearning.org/">IMLS</a>) used to have less of a problem, but since it has become more like NIPS over time, it has inherited this problem.  <a href="http://learningtheory.org/">COLT</a> has not suffered from this problem as much in my experience, although it had other problems related to the focus being defined too narrowly.  I do not have enough experience with UAI or KDD to comment there.</p>
<p>There are substantial flaws in the adversarial viewpoint.</p>
<ol>
<li>The adversarial viewpoint makes you stupid.  When viewed adversarially, any idea has crippling disadvantages and  no advantages.  Contorting your viewpoint enough to make this true damages your ability to conduct research.  In short, it promotes poor mental hygiene.</li>
<li>Many activities become impossible.  Doing research is in general extremely hard, so there are many instances where working with other people can allow you to do things which are otherwise impossible.</li>
<li>The previous two disadvantages apply even more strongly for a community&#8212;good ideas are more likely to be missed, change comes slowly, and often with steps backward.</li>
<li>At it&#8217;s most basic level, the assumption that research is zero-sum is flawed, because the process of research is not done in a closed system.  If the rest of society at large discovers that research is valuable, then the budget increases.</li>
</ol>
<p>Despite these disadvantages, there is a substantial advantage as well: you can materially protect and aid your career by rejecting papers, preventing grants, and generally discriminating against key people doing interesting but competitive work.</p>
<p>The adversarial viewpoint has a validity in proportion to the number of people subscribing to it.  For those of us who would like to deemphasize the adversarial viewpoint, what&#8217;s unclear is: how?   </p>
<p>One  concrete thing is: use <a href="http://arxiv.org/">Arxiv</a>.  For a long time, physicists have adopted an Arxiv-first philosophy, which I&#8217;ve come to respect.  Arxiv functions as a universal timestamp which decreases the power of an adversarial reviewer.  Essentially, you avoid giving away the power to muddy the track of invention.  I&#8217;m expecting to use Arxiv for essentially all my past-but-unpublished and future papers.</p>
<p>It is plausible that limiting the scope of bidding, as <a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a> suggested at the last ICML, and as is effectively implemented at this ICML, will help.  The system of review at journals might also help for the same reason.  In my experience as an author, if an anonymous reviewer wants to kill a paper they usually succeed. Most area chairs or program chairs are more interested in avoiding conflict with the reviewer (who they picked and may consider a friend) than reading the paper to determine the illogic of the review (which is a difficult task that simply cannot be done for all papers).  NIPS experimented with a reputation system for reviewers last year, but I&#8217;m unclear on how well it worked, as an author&#8217;s score for a review and a reviewer&#8217;s score for the paper may be deeply correlated, revealing little additional information.</p>
<p>Public discussion of research can help with this, because very poor logic simply doesn&#8217;t stand up under public scrutiny. While I hope to nudge people in this direction, it&#8217;s clear that most people aren&#8217;t yet comfortable with public discussion.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=499</wfw:commentRss>
		</item>
		<item>
		<title>Use of Learning Theory</title>
		<link>http://hunch.net/?p=496</link>
		<comments>http://hunch.net/?p=496#comments</comments>
		<pubDate>Tue, 23 Dec 2008 17:55:45 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Machine Learning]]></category>

		<category><![CDATA[Theory]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=496</guid>
		<description><![CDATA[I&#8217;ve had serious conversations with several people who believe that the theory in machine learning is &#8220;only useful for getting papers published&#8221;.  That&#8217;s a compelling statement, as I&#8217;ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance [...]]]></description>
			<content:encoded><![CDATA[<p>I&#8217;ve had serious conversations with several people who believe that the theory in machine learning is &#8220;only useful for getting papers published&#8221;.  That&#8217;s a compelling statement, as I&#8217;ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. </p>
<p>Naturally, I disagree and believe that learning theory has much more substantial applications.  </p>
<p>Even in core learning algorithm design, I&#8217;ve found learning theory to be useful, although it&#8217;s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:</p>
<ol>
<li>It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example I have of this is the <a href="http://waldron.stanford.edu/~isomap/">Isomap</a>, where the algorithm was informed by the <a href="http://waldron.stanford.edu/~isomap/BdSLT.pdf">analysis</a> yielding substantial improvements in sample complexity over earlier algorithmic ideas.</li>
<li>An algorithm with learning theory considered in it&#8217;s design can be more automatic. I&#8217;ve gained more respect for <a href="http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf">Rifkin&#8217;s claim</a>: that the one-against-all reduction, when tuned well, can often perform as well as other approaches.  The &#8220;when tuned well&#8221; caveat is however substantial, because learning algorithms may be applied by nonexperts or by other algorithms which are computationally constrained.  A reasonable and worthwhile hope for other methods of addressing multiclass problems is that they are more automatic and computationally faster.  The subtle issue here is: How do you measure &#8220;more automatic&#8221;?</li>
</ol>
<p>In my experience, learning theory is most useful in it&#8217;s crudest forms.  A good example comes in the architecting problem: how do you go about solving a learning problem?  I mean this in the broadest sense imaginable:</p>
<ol>
<li>Is it a learning problem or not?  Many problems are most easily solved via other means such as engineering, because that&#8217;s easier, because there is a severe data gathering problem, or because there is so much data that memorization works fine.  Learning theory such as statistical bounds and online learning with experts helps substantially here because it provides guidelines about what is possible to learn and what not.</li>
<li>What type of learning problem is it?  Is it a problem where exploration is required or not?  Is it a structured learning problem?  A multitask learning problem? A cost sensitive learning problem?  Are you interested in the median or the mean?  Is active learning useable or not?  Online or not?  Answering these questions correctly can easily make a difference between a succesful application and not.  Answering these questions is partly definition checking, and since the answer is often &#8220;all of the above&#8221;, figuring out which aspect of the problem to address first or next is helpful.
</li>
<li>What is the right learning algorithm to use?  Here the relative capacity of a learning algorithm and it&#8217;s computational efficiency are most important.  If you have few features and many examples, a nonlinear algorithm with more representational capacity is a good idea.  If you have many features and little data, linear representations or even exponentiated gradient style algorithms are important.  If you have very large amounts of data, the most scalable algorithms (so far) use a linear representation.  If you have little data and few features, a Bayesian approach may be your only option.  Learning theory can help in all of the above by quantifying &#8220;many&#8221;, &#8220;little&#8221;, &#8220;most&#8221;, and &#8220;few&#8221;.  How do you deal with the overfitting problem?  One thing I realized recently is that the overfitting problem can be a concern even with very large natural datasets, because some examples are naturally more important than others.
</li>
</ol>
<p>As might be clear, I think of learning theory as somewhat broader than might be traditional.  Some of this is simply education.  Many people have only been exposed to one piece of learning theory, often <a href="http://en.wikipedia.org/wiki/Vapnik-Chervonenkis_theory">VC theory</a> or it&#8217;s cousins.  After seeing this, they come to think of it as learning theory.  VC theory is a good theory, but it is not complete, and other elements of learning theory seem at least as important and useful.   Another aspect is publishability.  Simply sampling from the learning theory in existing papers does not necessarily give a good distribution of subjects for teaching, because the goal of impressing reviewers does not necessarily coincide with the clean simple analysis that is teachable.</p>
<p>(*) There is significant investigation into improving the tightness of bounds to the point of usefulness, and maybe it will pay off.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=496</wfw:commentRss>
		</item>
		<item>
		<title>Summer Conferences</title>
		<link>http://hunch.net/?p=485</link>
		<comments>http://hunch.net/?p=485#comments</comments>
		<pubDate>Sat, 13 Dec 2008 00:35:36 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Conferences]]></category>

		<category><![CDATA[Machine Learning]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=485</guid>
		<description><![CDATA[Here&#8217;s a handy table for the summer conferences.


Conference
Deadline
Reviewer Targeting
Double Blind
Author Feedback
Location
Date


ICML (wrong ICML)
January 26
Yes
Yes
Yes
Montreal, Canada
June 14-17


COLT
February 13
No
No
Yes
Montreal
June 19-21


UAI
March 13
No
Yes
No
Montreal
June 19-21


KDD
February 2/6
No
No
No
Paris, France
June 28-July 1


Reviewer targeting is new this year.  The idea is that many poor decisions happen because the papers go to reviewers who are unqualified, and the hope is that allowing authors to [...]]]></description>
			<content:encoded><![CDATA[<p>Here&#8217;s a handy table for the summer conferences.</p>
<table border=1>
<tr>
<td>Conference</td>
<td>Deadline</td>
<td>Reviewer Targeting</td>
<td>Double Blind</td>
<td>Author Feedback</td>
<td>Location</td>
<td>Date</td>
</tr>
<tr>
<td><a href="http://www.cs.mcgill.ca/~icml2009/index.html">ICML</a> (<a href="http://www.icml2009.com/">wrong ICML</a>)</td>
<td>January 26</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Montreal, Canada</td>
<td>June 14-17</td>
</tr>
<tr>
<td><a href="http://www.cs.mcgill.ca/~colt2009/">COLT</a></td>
<td>February 13</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Montreal</td>
<td>June 19-21</td>
</tr>
<tr>
<td><a href="http://www.cs.mcgill.ca/~uai2009/">UAI</a></td>
<td>March 13</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Montreal</td>
<td>June 19-21</td>
</tr>
<tr>
<td><a href="http://www.sigkdd.org/kdd2009/">KDD</a></td>
<td>February 2/6</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Paris, France</td>
<td>June 28-July 1</td>
</tr>
</table>
<p>Reviewer targeting is new this year.  The idea is that many poor decisions happen because the papers go to reviewers who are unqualified, and the hope is that allowing authors to point out who is qualified results in better decisions.  In my experience, this is a reasonable idea to test.</p>
<p>Both UAI and COLT are experimenting this year as well with double blind and author feedback, respectively.  Of the two, I believe author feedback is more important, as I&#8217;ve seen it make a difference.  However, I still consider double blind reviewing a net win, as it&#8217;s a substantial public commitment to fairness.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=485</wfw:commentRss>
		</item>
		<item>
		<title>A NIPS paper</title>
		<link>http://hunch.net/?p=482</link>
		<comments>http://hunch.net/?p=482#comments</comments>
		<pubDate>Mon, 08 Dec 2008 01:46:22 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Bayesian]]></category>

		<category><![CDATA[Empirical]]></category>

		<category><![CDATA[Machine Learning]]></category>

		<category><![CDATA[Papers]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=482</guid>
		<description><![CDATA[I&#8217;m skipping NIPS this year in favor of Ada, but I wanted to point out this paper by Andriy Mnih and Geoff Hinton.  The basic claim of the paper is that by carefully but automatically constructing a binary tree over words, it&#8217;s possible to predict words well with huge computational resource savings over unstructured [...]]]></description>
			<content:encoded><![CDATA[<p>I&#8217;m skipping NIPS this year in favor of <a href="http://hunch.net/~ada">Ada</a>, but I wanted to point out <a href="http://www.cs.toronto.edu/~amnih/papers/hlbl_draft.pdf">this paper</a> by <a href="http://www.cs.toronto.edu/~amnih/">Andriy Mnih</a> and <a href="http://www.cs.toronto.edu/~hinton/">Geoff Hinton</a>.  The basic claim of the paper is that by carefully but automatically constructing a binary tree over words, it&#8217;s possible to predict words well with huge computational resource savings over unstructured approaches.</p>
<p>I&#8217;m interested in this beyond the application to word prediction because it is relevant to the general normalization problem: If you want to predict the probability of one of a large number of events, often you must compute a predicted score for all the events and then normalize, a computationally inefficient operation.  The problem comes up in many places using probabilistic models, but I&#8217;ve run into it with high-dimensional regression.</p>
<p>There are a couple workarounds for this computational bug:</p>
<ol>
<li>Approximate. There are many ways.  Often the approximations are uncontrolled (i.e. can be arbitrarily bad), and hence finicky in application.</li>
<li>Avoid.  You don&#8217;t really want a probability, you want the most probable choice which can be found more directly.  <a href="http://www.cs.nyu.edu/~yann/research/ebm/">Energy based model</a> update rules are an example of that approach and there are many other direct methods from supervised learning.  This is great when it applies, but sometimes a probability is actually needed.</li>
</ol>
<p>This paper points out that a third approach can be viable empirically: use a self-normalizing structure.  It seems highly likely that this is true in other applications as well.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=482</wfw:commentRss>
		</item>
		<item>
		<title>A Bumper Crop of Machine Learning Graduates</title>
		<link>http://hunch.net/?p=476</link>
		<comments>http://hunch.net/?p=476#comments</comments>
		<pubDate>Sat, 29 Nov 2008 01:26:52 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Machine Learning]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=476</guid>
		<description><![CDATA[My impression is that this is a particularly strong year for machine learning graduates.  Here&#8217;s my short list of the strong graduates I know.  Analpha (for perversity&#8217;s sake) by last name:

Jenn Wortmann. When Jenn visited us for the summer, she had one, two, three, four papers.  That is typical&#8212;she&#8217;s smart, capable, and [...]]]></description>
			<content:encoded><![CDATA[<p>My impression is that this is a particularly strong year for machine learning graduates.  Here&#8217;s my short list of the strong graduates I know.  Analpha (for perversity&#8217;s sake) by last name:</p>
<ol>
<li><a href="http://www.seas.upenn.edu/~wortmanj/">Jenn Wortmann</a>. When Jenn visited us for the summer, she had <a href="http://www.seas.upenn.edu/~wortmanj/papers/scavenging.pdf">one</a>, <a href="http://www.seas.upenn.edu/~wortmanj/papers/wagering.pdf">two</a>, <a href="http://www.seas.upenn.edu/~wortmanj/papers/lmsrcomplexity.pdf">three</a>, <a href="http://www.seas.upenn.edu/~wortmanj/papers/explore.pdf">four</a> papers.  That is typical&#8212;she&#8217;s smart, capable, and follows up many directions of research.  I believe approximately all of her many papers are on different subjects.</li>
<li><a href="http://www.cs.toronto.edu/~rsalakhu/">Ruslan Salakhutdinov</a>. A <a href="http://www.sciencemag.org/cgi/content/short/313/5786/504">Science paper on bijective dimensionality reduction</a>, mastered and improved on deep belief nets which seems like an important flavor of nonlinear learning, and in my experience he&#8217;s very fast, capable and creative at problem solving.</li>
<li><a href="http://www.cs.nyu.edu/~ranzato/">Marc&#8217;Aurelio Ranzato</a>.  I haven&#8217;t spoken with Marc very much, but he had a great visit at Yahoo! this summer, and has an impressive portfolio of applications and improvements on convolutional neural networks and other deep learning algorithms.</li>
<li><a href="http://www.research.rutgers.edu/~lihong/">Lihong Li</a>.  Lihong developed the <a href="http://www.research.rutgers.edu/~lihong/pub/Li08Knows.pdf">KWIK (&#8221;Knows what it Knows&#8221;) learning framework</a>, for analyzing and creating uncertainty-aware learning algorithms. New mathematical models of learning are rare, and the topic is of substantial interest, so this is pretty cool.  He&#8217;s also worked on a wide variety of other subjects and in my experience is broadly capable.</li>
<li><a href="http://www.cs.cmu.edu/~shanneke/">Steve Hanneke</a>: When the chapter on active learning is written in a machine learning textbook, I expect the <a href="http://www.cs.cmu.edu/~shanneke/docs/2007/hanneke-agnostic-active.pdf">disagreement coefficient</a> to be in it.  Steve&#8217;s work is strongly distinguished from his adviser&#8217;s, so he is guaranteed capable of independent research.</li>
</ol>
<p>There are a couple others such as <a href="http://www.cs.ucsd.edu/~djhsu/">Daniel</a> and <a href="http://www.cs.berkeley.edu/~jake/">Jake</a> for whom I&#8217;m unsure of their graduation plans, although they have already done good work.  In addition, I&#8217;m sure there are several others that I don&#8217;t know&#8212;feel free to mention others I don&#8217;t know in comments.</p>
<p>It&#8217;s traditional to imagine that one is best overall for hiring purposes, but I have substantial difficulty with that&#8212;the field of ML is simply to broad.  Instead, if you are interested in hiring, each should be considered in your context.</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=476</wfw:commentRss>
		</item>
		<item>
		<title>Efficient Reinforcement Learning in MDPs</title>
		<link>http://hunch.net/?p=472</link>
		<comments>http://hunch.net/?p=472#comments</comments>
		<pubDate>Wed, 26 Nov 2008 13:29:47 +0000</pubDate>
		<dc:creator>jl</dc:creator>
		
		<category><![CDATA[Reinforcement]]></category>

		<category><![CDATA[Theory]]></category>

		<guid isPermaLink="false">http://hunch.net/?p=472</guid>
		<description><![CDATA[Claude Sammut is attempting to put together an Encyclopedia of Machine Learning.  I volunteered to write one article on Efficient RL in MDPs, which I would like to invite comment on.  Is something critical missing?
]]></description>
			<content:encoded><![CDATA[<p><a href="http://www.cse.unsw.edu.au/~claude/">Claude Sammut</a> is attempting to put together an <a href="http://www.amazon.co.uk/Encyclopedia-of-Machine-Learning/dp/0387307680">Encyclopedia of Machine Learning</a>.  I volunteered to write one article on <a href="images/Efficient_Reinforcement_Learning.pdf">Efficient RL in MDPs</a>, which I would like to invite comment on.  Is something critical missing?</p>
]]></content:encoded>
			<wfw:commentRss>http://hunch.net/?feed=rss2&amp;p=472</wfw:commentRss>
		</item>
	</channel>
</rss>
