<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="http://feeds.feedburner.com/~d/styles/atom10full.xsl" type="text/xsl" media="screen"?><?xml-stylesheet href="http://feeds.feedburner.com/~d/styles/itemcontent.css" type="text/css" media="screen"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"><id>tag:blogger.com,1999:blog-8898949683610477251</id><updated>2009-01-17T19:28:46.717Z</updated><title type="text">Tom White</title><subtitle type="html">&lt;pre&gt;Problems worthy
  of attack
prove their worth
  by hitting back.
-- Piet Hein&lt;/pre&gt;</subtitle><link rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml" href="http://www.lexemetech.com/feeds/posts/default" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/" /><link rel="next" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default?start-index=26&amp;max-results=25&amp;redirect=false&amp;v=2" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><generator version="7.00" uri="http://www.blogger.com">Blogger</generator><openSearch:totalResults>43</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><link rel="self" href="http://feeds.feedburner.com/TomWhite" type="application/atom+xml" /><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-5053020334356534059</id><published>2008-11-20T10:39:00.005Z</published><updated>2008-11-20T10:49:06.393Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-11-20T10:49:06.393Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Cloudera" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop Developer Zeitgeist</title><content type="html">The Cloudera team have just released a &lt;a href="http://community.cloudera.com/"&gt;website&lt;/a&gt; which has a few reports on various Hadoop development metrics. I like the &lt;a href="http://community.cloudera.com/reports/2/issues/"&gt;Most Watched Open Jira Issues&lt;/a&gt;, as it gives a good summary of what Hadoop Core developers are thinking about.&lt;br /&gt;&lt;br /&gt;Personally, I can't wait for the new MapReduce API (&lt;a href="http://issues.apache.org/jira/browse/HADOOP-1230"&gt;HADOOP-1230&lt;/a&gt;), which is currently the third most watched issue.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/459440050" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/5053020334356534059/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=5053020334356534059" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5053020334356534059?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5053020334356534059?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/11/hadoop-developer-zeitgeist.html" title="Hadoop Developer Zeitgeist" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-1310920520805089454</id><published>2008-10-16T11:36:00.003+01:00</published><updated>2008-10-16T11:47:22.559+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-10-16T11:47:22.559+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Cloudera" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Cloudera</title><content type="html">I'm pleased to announce that I've joined &lt;a href="http://www.cloudera.com/"&gt;Cloudera&lt;/a&gt;, a new startup providing support for Hadoop. Amr Awadallah (who's one of the founders) has got more details in his &lt;a href="http://www.awadallah.com/blog/2008/10/13/the-startup-is-cloudera-the-business-is-hadoop-mapreduce/"&gt;blog post&lt;/a&gt;.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/422531931" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/1310920520805089454/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=1310920520805089454" title="3 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1310920520805089454?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1310920520805089454?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/10/cloudera.html" title="Cloudera" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-1667539494188985504</id><published>2008-09-16T11:26:00.002+01:00</published><updated>2008-09-16T18:44:30.610+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-09-16T18:44:30.610+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Book" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop: The Definitive Guide</title><content type="html">&lt;span style="font-weight:bold;"&gt;Update:&lt;/span&gt; Fixed feedback link.&lt;br /&gt;&lt;br /&gt;The Rough Cut of &lt;a href="http://oreilly.com/catalog/9780596521998/index.html"&gt;Hadoop: The Definitive Guide&lt;/a&gt; is now up on O'Reilly's site. There are a few chapters available already, at various stages of completion. Remember, it's still pretty rough. I'd love to hear any suggestions for improvements that you may have though. You can submit feedback from &lt;a href="http://safari.oreilly.com/9780596521974"&gt;Safari&lt;/a&gt; where the book is hosted. As the &lt;a href="http://oreilly.com/roughcuts/faq.csp"&gt;Rough Cuts FAQ&lt;/a&gt; explains, I'd like feedback on missing topics, if something is not understandable, and technical mistakes.&lt;br /&gt;&lt;br /&gt;Now I just need to go and write the rest of it...&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/394080092" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/1667539494188985504/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=1667539494188985504" title="2 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1667539494188985504?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1667539494188985504?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/09/hadoop-definitive-guide.html" title="Hadoop: The Definitive Guide" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">2</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7867943272436546694</id><published>2008-09-04T22:00:00.004+01:00</published><updated>2008-09-12T20:46:21.241+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-09-12T20:46:21.241+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Amazon S3" /><category scheme="http://www.blogger.com/atom/ns#" term="Data" /><category scheme="http://www.blogger.com/atom/ns#" term="Amazon EC2" /><title type="text">Hosting Large Public Datasets on Amazon S3</title><content type="html">&lt;span style="font-style:italic;"&gt;&lt;span style="font-weight:bold;"&gt;Update&lt;/span&gt;: I just thought of a quick and dirty way of doing this: just store your content on an extra large EC2 instance (holds up to 1690GB) and make the image public. Anyone can access it using their EC2 account, you just get charged for hosting the image.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;There's a great deal of interest in large, publicly available datasets (see, for example, &lt;a href="http://groups.google.com/group/get-theinfo/browse_thread/thread/79e5b1159e533d52"&gt;this thread&lt;/a&gt; from &lt;a href="http://theinfo.org/"&gt;theinfo.org&lt;/a&gt;), but for very large datasets it is still expensive to provide the bandwidth to distribute them. Imagine if you could get your hands on the data from a large web crawl, the kind of thing that the &lt;a href="http://www.archive.org/"&gt;Internet Archive&lt;/a&gt; produces. I'm sure people would discover some interesting things from it.&lt;br /&gt;&lt;br /&gt;&lt;a href="http://aws.amazon.com/s3"&gt;Amazon S3&lt;/a&gt; is an obvious choice for storing data for public consumption, but while the cost for storage may be reasonable, the cost for transfer can be crippling since the cost is not under the control of the data provider, being incurred &lt;span style="font-style: italic;"&gt;for each transfer&lt;/span&gt; (which is initiated by the user).&lt;br /&gt;&lt;br /&gt;For example, consider a 1TB dataset. With storage running at $0.15 per GB per month this works out at around $150 per month to host. With transfer costs costing $0.18 per GB, this dataset costs around $180 for each transfer out of Amazon! It's not surprising large datasets are not publicly hosted on S3.&lt;br /&gt;&lt;br /&gt;However, transferring data between S3 and EC2 is free, so could we limit transfers from S3 so they are only possible to EC2? You (or anyone else) could run an analysis on EC2 (using Hadoop, say) and only pay for the EC2 time. Or you could transfer it out of EC2 at your own expense. S3 doesn't support this option directly, but it is possible to emulate it with a bit of code.&lt;br /&gt;&lt;br /&gt;The idea (suggested by &lt;a href="http://blog.lucene.com/"&gt;Doug Cutting&lt;/a&gt;) is to make objects private on S3 to restrict access generally, then run a proxy on EC2 that is authorized to access the objects. The proxy only accepts connections from within EC2: any client that is outside Amazon's cloud is firewalled out. This combination ensures only EC2 instances can access the S3 objects, thus removing any bandwidth costs.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Implementation&lt;/h3&gt;I've written such a proxy. It's a Java servlet that uses the &lt;a href="https://jets3t.dev.java.net/"&gt;JetS3t&lt;/a&gt; library to add the correct &lt;a href="http://docs.amazonwebservices.com/AmazonS3/2006-03-01/index.html?RESTAuthentication.html"&gt;Amazon S3 &lt;code&gt;Authorization&lt;/code&gt; HTTP header&lt;/a&gt; to gain access to the owner's objects on S3. If the proxy is running on the EC2 instance with hostname &lt;span style="font-style: italic;"&gt;ec2-67-202-43-67.compute-1.amazonaws.com&lt;/span&gt;, for example, then a request for&lt;br /&gt;&lt;pre&gt;http://ec2-67-202-43-67.compute-1.amazonaws.com/bucket/object&lt;br /&gt;&lt;/pre&gt;is proxied to the protected object at&lt;br /&gt;&lt;pre&gt;http://s3.amazonaws.com/bucket/object&lt;br /&gt;&lt;/pre&gt;To ensure that only clients on EC2 can get access to the proxy I set up an EC2 security group (which limits access to port 80):&lt;br /&gt;&lt;pre&gt;ec2-add-group ec2-private-subnet -d "Group for all Amazon EC2 instances."&lt;br /&gt;ec2-authorize ec2-private-subnet -p 80 -s 10.0.0.0/8&lt;/pre&gt;Then by launching the proxy in this group, only machines on EC2 can connect. (Initially, I thought I had to add public IP addresses to the group -- which, incidentally, I found in &lt;a href="http://developer.amazonwebservices.com/connect/thread.jspa?messageID=51028"&gt;this forum posting&lt;/a&gt; -- but this is not necessary as the public DNS name of an EC2 instance resolves to the private IP address within EC2.) The AWS credentials to gain access to the S3 objects are passed in the user data, along with the hostname of S3:&lt;br /&gt;&lt;pre&gt;ec2-run-instances -k gsg-keypair -g ec2-private-subnet \&lt;br /&gt;-d "&amp;lt;aws_access_key&amp;gt; &amp;lt;aws_secret_key&amp;gt; s3.amazonaws.com" ami-fffd1996&lt;/pre&gt;This AMI (ID &lt;code&gt;ami-fffd1996&lt;/code&gt;) is publicly available, so anyone can use it by using the commands shown here. (The code is available &lt;a href="http://s3.amazonaws.com/s3proxy/s3proxy-0.1.tar.gz"&gt;here&lt;/a&gt;, under an Apache 2.0 license, but you don't need this if you only intend to run or use a proxy.)&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Demo&lt;/h3&gt;Here's a resource on S3 that is protected: &lt;span style="font-style:italic;"&gt;http://s3.amazonaws.com/tiling/private.txt&lt;/span&gt;. When you try to retrieve it you get an authorization error:&lt;br /&gt;&lt;pre&gt;% &lt;span style="font-weight: bold;"&gt;curl http://s3.amazonaws.com/tiling/private.txt&lt;/span&gt;&lt;br /&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;&lt;br /&gt;&amp;lt;Error&amp;gt;&lt;br /&gt;&amp;lt;Code&amp;gt;AccessDenied&amp;lt;/Code&amp;gt;&lt;br /&gt;&amp;lt;Message&amp;gt;Access Denied&amp;lt;/Message&amp;gt;&lt;br /&gt;&amp;lt;RequestId&amp;gt;57E370CDDD9FE044&amp;lt;/RequestId&amp;gt;&lt;br /&gt;&amp;lt;HostId&amp;gt;dA+9II1dYAjPE5aNsnRxhVoQ5qy3KCa6frkLg3SyTwzP3i2SQNCU534/v8NXXEnN&amp;lt;/HostId&amp;gt;&lt;br /&gt;&amp;lt;/Error&amp;gt;&lt;/pre&gt;With a proxy running, I still can't retrieve the resource via the proxy from outside EC2. It just times out due to the firewall rule:&lt;br /&gt;&lt;pre&gt;% &lt;span style="font-weight: bold;"&gt;curl http://ec2-67-202-56-11.compute-1.amazonaws.com/tiling/private.txt&lt;/span&gt;&lt;br /&gt;curl: (7) couldn't connect to host&lt;/pre&gt;But it does works from an EC2 machine (any EC2 machine):&lt;br /&gt;&lt;pre&gt;% &lt;span style="font-weight: bold;"&gt;curl http://ec2-67-202-56-11.compute-1.amazonaws.com/tiling/private.txt&lt;/span&gt;&lt;br /&gt;secret&lt;/pre&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;By running a proxy on EC2, at 10 cents per hour (small instance) - or $72 a month, you can allow folks using EC2 to access your data on S3 for free. While running the proxy is not free, it is a fixed cost that might be acceptable to some organizations, particularly those that have an interest in making data publicly available (but can't stomach large bandwidth costs).&lt;br /&gt;&lt;br /&gt;A few questions:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Is this useful?&lt;/li&gt;&lt;li&gt;Is there a better way of doing it?&lt;/li&gt;&lt;li&gt;Can we have this built into S3 (please, Amazon)?&lt;/li&gt;&lt;/ul&gt;&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/383580359" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7867943272436546694/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7867943272436546694" title="4 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7867943272436546694?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7867943272436546694?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/09/hosting-large-public-datasets-on-amazon.html" title="Hosting Large Public Datasets on Amazon S3" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7570203001448264185</id><published>2008-08-23T21:39:00.002+01:00</published><updated>2008-08-23T21:41:35.517+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-08-23T21:41:35.517+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Amazon Web Services" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Elastic Hadoop Clusters with Amazon's Elastic Block Store</title><content type="html">I gave a &lt;a href="http://skillsmatter.com/podcast/cloud-grid/hadoop-on-amazon-s3ec2"&gt;talk&lt;/a&gt; on Tuesday at the first &lt;a href="http://skillsmatter.com/event/java-jee/hadoop-user-group-meeting"&gt;Hadoop User Group UK&lt;/a&gt; about Hadoop and Amazon Web services - how and why you can run Hadoop with AWS. I mentioned how integrating Hadoop with Amazon's "Persistent local storage", which Werner Vogels had &lt;a href="http://www.allthingsdistributed.com/2008/04/persistent_storage_for_amazon.html"&gt;pre-announced&lt;/a&gt; in April, would be a great feature to have to enable truly elastic Hadoop clusters that you could stop and start on demand.&lt;br /&gt;&lt;br /&gt;Well, the very next day Amazon launched this service, called &lt;a href="http://www.amazon.com/b/ref=sc_fe_c_0_201590011_1?ie=UTF8&amp;amp;node=689343011&amp;amp;no=201590011"&gt;Elastic Block Store&lt;/a&gt; (EBS). So in this post I thought I'd sketch out how an elastic Hadoop might work.&lt;br /&gt;&lt;br /&gt;A bit of background. Currently there are three main ways to use Hadoop with AWS:&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;1. MapReduce with S3 source and sink&lt;/h3&gt;&lt;br /&gt;&lt;a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7wd4umVGI/AAAAAAAAAE4/pnzP5XjfjtI/s1600-h/s3-mapred.png"&gt;&lt;img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7wd4umVGI/AAAAAAAAAE4/pnzP5XjfjtI/s320/s3-mapred.png" alt="" id="BLOGGER_PHOTO_ID_5237387812913173602" border="0"&gt;&lt;/a&gt;In this set up, the data resides on S3, and the MapReduce daemons run on a temporary EC2 cluster for the duration of the job run. This works, and is especially convenient if you've already store your data on S3, but you don't get any data locality. Data locality is what enables the magic of MapReduce to work efficiently - the computation is scheduled to run on the machine where the data is stored, so you get huge savings in not having to ship terabytes of data around the network. EC2 does not share nodes with S3 storage, in fact they are often in different data centres, so performance is nowhere near as good as a regular Hadoop cluster where the data in stored in HDFS (see 3. below).&lt;br /&gt;&lt;br /&gt;It's not all doom and gloom, as the bandwidth between EC2 and S3 is actually pretty good, as Rightscale &lt;a href="http://blog.rightscale.com/2007/10/28/network-performance-within-amazon-ec2-and-to-amazon-s3/"&gt;found when they did some measurements&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;2. MapReduce from S3 with HDFS staging&lt;/h3&gt;&lt;br /&gt;&lt;a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://1.bp.blogspot.com/_IhqEHw4_Ick/SK7wv4AZxeI/AAAAAAAAAFA/66XE-v6mgFE/s1600-h/s3-hdfs-mapred.png"&gt;&lt;img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="http://1.bp.blogspot.com/_IhqEHw4_Ick/SK7wv4AZxeI/AAAAAAAAAFA/66XE-v6mgFE/s320/s3-hdfs-mapred.png" alt="" id="BLOGGER_PHOTO_ID_5237388121957058018" border="0"&gt;&lt;/a&gt;Data is stored on S3 but copied to a temporary HDFS cluster running on EC2. This is just a variation of the previous set-up, which is good if you want to run several jobs against the same input data. You save by only copying the data across the network once, but you pay a little more due to HDFS replication.&lt;br /&gt;&lt;br /&gt;The bottleneck is still copying the data out of S3. (Copying the results back into S3 isn't usually as bad as the output is often an order or two of magnitude smaller than the input.)&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;3. HDFS on Amazon EC2&lt;/h3&gt;&lt;br /&gt;&lt;a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7w--YOZvI/AAAAAAAAAFI/iu8En_A_Arw/s1600-h/ec2.png"&gt;&lt;img style="margin: 0pt 10px 10px 0pt; float: right; cursor: pointer;" src="http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7w--YOZvI/AAAAAAAAAFI/iu8En_A_Arw/s320/ec2.png" alt="" id="BLOGGER_PHOTO_ID_5237388381365626610" border="0"&gt;&lt;/a&gt;Of course, you could just run a Hadoop cluster on EC2 and store your data there (and not in S3). In this scenario, you are committed to running your EC2 cluster long term, which can prove expensive, although the locality is excellent.&lt;br /&gt;&lt;br /&gt;These three scenarios demonstrate that you pay for locality. However, there is a gulf between S3 and local disks that EBS fills nicely. EBS does not have the bandwidth of local disks, but it's significantly better than S3. Rightscale &lt;a href="http://blog.rightscale.com/2008/08/20/amazon-ebs-explained/"&gt;again&lt;/a&gt;:&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;The bottom line though is that performance exceeds what we’ve seen for filesystems striped across the four local drives of x-large instances.&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;h3&gt;Implementing Elastic Hadoop&lt;/h3&gt;&lt;br /&gt;&lt;a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://3.bp.blogspot.com/_IhqEHw4_Ick/SK7xRq_YwjI/AAAAAAAAAFY/0ypyV3MrprQ/s1600-h/ec2-with-storage-volumes.png"&gt;&lt;img style="margin: 0pt 0pt 10px 10px; float: right; cursor: pointer;" src="http://3.bp.blogspot.com/_IhqEHw4_Ick/SK7xRq_YwjI/AAAAAAAAAFY/0ypyV3MrprQ/s320/ec2-with-storage-volumes.png" alt="" id="BLOGGER_PHOTO_ID_5237388702578688562" border="0"&gt;&lt;/a&gt;The main departure from the current Hadoop on EC2 approach is the need to maintain a map from storage volume to node type: i.e. we need to remember which volume is a master volume (storing the namenode's data) and which is a worker volume (storing the datanode's data). It would be nice if you could just start up EC2 instances for all the volumes, and have them figure out which is which, but this might not work as the master needs to be started first so its address can be given to the workers in their user data. (This choreography problem could be solved by introducing ZooKeeper, but that's another story.) So for a first cut, we could simply keep two files (locally, or on S3 or even SimpleDB) called &lt;font style="font-style: italic;"&gt;master-volumes&lt;/font&gt;, and &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt;, which simply list the volume IDs for each node type, one per line.&lt;br /&gt;&lt;br /&gt;Assume there is one master running the namenode and jobtracker, and &lt;font style="font-style: italic;"&gt;n&lt;/font&gt; worker nodes each running a datanode and tasktracker.&lt;br /&gt;&lt;br /&gt;To create a new cluster&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Create &lt;font style="font-style: italic;"&gt;n&lt;/font&gt; + 1 volumes.&lt;/li&gt;&lt;li&gt;Create the &lt;font style="font-style: italic;"&gt;master-volumes&lt;/font&gt; file and write the first volume ID into it.&lt;/li&gt;&lt;li&gt;Create the &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt; file and write the remaining volume IDs to it.&lt;/li&gt;&lt;li&gt;Follow the steps for starting a cluster.&lt;/li&gt;&lt;/ol&gt;To start a cluster&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Start the master instance passing it the &lt;font style="font-style: italic;"&gt;master-volumes&lt;/font&gt; as user data. On startup the instance attaches to the volume it was told to. It then formats the namenode if it isn't already formatted, then starts the namenode, secondary namenode and jobtracker.&lt;/li&gt;&lt;li&gt;Start &lt;span style="font-style: italic;"&gt;n&lt;/span&gt; worker instances passing it the &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt; as user data. On startup each instance attaches to the volume on line &lt;font style="font-style: italic;"&gt;i&lt;/font&gt;, where &lt;font style="font-style: italic;"&gt;i&lt;/font&gt; is the &lt;font face="courier new"&gt;ami-launch-index&lt;/font&gt; of the instance. Each instance then starts a datanode and tasktacker.&lt;/li&gt;&lt;li&gt;If any worker instances failed to start then launch them again.&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;To shutdown a cluster&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Shutdown the Hadoop cluster daemons.&lt;/li&gt;&lt;li&gt;Detach the EBS volumes.&lt;/li&gt;&lt;li&gt;Shutdown the EC2 instances.&lt;/li&gt;&lt;/ol&gt;To grow a cluster&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Create &lt;font style="font-style: italic;"&gt;m&lt;/font&gt; new volumes, where &lt;font style="font-style: italic;"&gt;m&lt;/font&gt; is the size to grow by.&lt;/li&gt;&lt;li&gt;Append the &lt;font style="font-style: italic;"&gt;m&lt;/font&gt; new volume IDs to the &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt; file.&lt;/li&gt;&lt;li&gt;Start &lt;span style="font-style: italic;"&gt;m&lt;/span&gt; worker instances passing it the &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt; as user data. On startup each instance attaches to the volume on line &lt;font style="font-style: italic;"&gt;n + i&lt;/font&gt;, where &lt;font style="font-style: italic;"&gt;i&lt;/font&gt; is the &lt;font face="courier new"&gt;ami-launch-index&lt;/font&gt; of the instance. Each instance then starts a datanode and tasktacker.&lt;/li&gt;&lt;/ol&gt;Future enhancements: attach multiple volumes for performance/storage growth on the namenode, or resilience on the namenode; integrate the secondary namenode backup facility with EBS snapshots to S3; provide tools for managing the &lt;font style="font-style: italic;"&gt;worker-volumes&lt;/font&gt; file (for example, integrating with datanode decommissioning).&lt;br /&gt;&lt;br /&gt;Building this would be a great project to work on - I hope someone does it!&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/372962068" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7570203001448264185/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7570203001448264185" title="5 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7570203001448264185?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7570203001448264185?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/08/elastic-hadoop-clusters-with-amazons.html" title="Elastic Hadoop Clusters with Amazon's Elastic Block Store" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://2.bp.blogspot.com/_IhqEHw4_Ick/SK7wd4umVGI/AAAAAAAAAE4/pnzP5XjfjtI/s72-c/s3-mapred.png" height="72" width="72" /><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">5</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-1050541487495156879</id><published>2008-07-23T20:26:00.002+01:00</published><updated>2008-07-23T22:18:34.869+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-07-23T22:18:34.869+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Pluggable Hadoop</title><content type="html">&lt;span style="font-weight:bold;"&gt;Update&lt;/span&gt;: This quote from Tim O'Reilly in his OSCON keynote today sums up the changes I describe below: "Do less and then create extensibility mechanisms." (via &lt;a href="http://raibledesigns.com/rd/entry/oscon_2008_the_keynote"&gt;Matt Raible&lt;/a&gt;)&lt;br /&gt;&lt;br /&gt;I'm noticing an increased desire to make Hadoop more modular. I'm not sure why this is happening now, but it's probably because as more people start using Hadoop it needs to be more malleable (people want to plug in their own implementations of things), and the way to do that in software is through modularity.&lt;br /&gt;&lt;br /&gt;Some examples:&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Job scheduling&lt;/h3&gt;The current scheduler is a simple FIFO scheduler which is adequate for small clusters with a few cooperating users. On larger clusters the best advice has been to use &lt;a href="http://hadoop.apache.org/core/docs/current/hod.html"&gt;HOD&lt;/a&gt; (Hadoop On Demand), but that has its own problems with inefficient cluster utilization. This situation led to a number of proposals to make the scheduler pluggable (&lt;a href="https://issues.apache.org/jira/browse/HADOOP-2510"&gt;HADOOP-2510&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3412"&gt;HADOOP-3412&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3444"&gt;HADOOP-3444&lt;/a&gt;). Already there is a &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3746"&gt;fair scheduler implementation&lt;/a&gt; (like the &lt;a href="http://en.wikipedia.org/wiki/Completely_Fair_Scheduler"&gt;Completely Fair Scheduler&lt;/a&gt; in Linux) from Facebook.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;HDFS block placement&lt;/h3&gt;Today the algorithm for placing a file's blocks across datanodes in the cluster is hardcoded into HDFS, and while it has &lt;a href="https://issues.apache.org/jira/browse/HADOOP-2559"&gt;evolved&lt;/a&gt;, it is clear that a one-size-fits-all approach is not necessarily the best approach. Hence the new proposal to support &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3799"&gt;pluggable block placement algorithms&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Instrumentation&lt;/h3&gt;Finding out what is happening in a distributed system is a hard problem. Today, Hadoop has a &lt;a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/metrics/package-summary.html"&gt;metrics API&lt;/a&gt; (for gathering statistics from the main components of Hadoop), but there is interest in adding other logging systems, such as &lt;a href="http://radlab.cs.berkeley.edu/wiki/Projects/X-Trace_on_Hadoop"&gt;X-Trace&lt;/a&gt;, via a new &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3772"&gt;instrumentation API&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Serialization&lt;/h3&gt;The ability to use &lt;a href="https://issues.apache.org/jira/browse/HADOOP-1986"&gt;pluggable serialization frameworks&lt;/a&gt; in MapReduce appeared in Hadoop 0.17.0, but has received renewed interest due to the talk around &lt;a href="http://incubator.apache.org/thrift/"&gt;Apache Thrift&lt;/a&gt; and &lt;a href="http://code.google.com/p/protobuf/"&gt;Google Protocol Buffers&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Component lifecycle&lt;/h3&gt;There is work being done to &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3628"&gt;add a lifecyle interface to Hadoop components&lt;/a&gt;. One of the goals is to make it easier to subclass components, so they can be customized.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Remove dependency cycles&lt;/h3&gt;This is really just good engineering practice, but the existence of dependencies makes it harder to understand, modify and extend code. Bill de hÓra did a great &lt;a href="http://www.dehora.net/journal/2008/07/06/3-12-minutes-to-sort-a-terabyte-hadoops-code-structure/"&gt;analysis&lt;/a&gt; of Hadoop's code structure (and its deficiencies), which has lead to some work to enforce module dependencies and remove the cycles.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/343867607" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/1050541487495156879/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=1050541487495156879" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1050541487495156879?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/1050541487495156879?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/07/pluggable-hadoop.html" title="Pluggable Hadoop" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-2466668333674859777</id><published>2008-07-08T10:43:00.012+01:00</published><updated>2008-07-08T14:03:40.052+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-07-08T14:03:40.052+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Thrift" /><category scheme="http://www.blogger.com/atom/ns#" term="Serialization" /><category scheme="http://www.blogger.com/atom/ns#" term="RPC" /><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">RPC and Serialization with Hadoop, Thrift, and Protocol Buffers</title><content type="html">Hadoop and related projects like Thrift provide a choice of protocols and formats for doing RPC and serialization. In this post I'll briefly run through them and explain where they came from, how they relate to each other and how Google's &lt;a href="http://google-code-updates.blogspot.com/2008/07/protocol-buffers-our-serialized.html"&gt;newly released Protocol Buffers&lt;/a&gt; might fit in.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;RPC and Writables&lt;/h3&gt;Hadoop has its own RPC mechanism that dates back to when Hadoop was a part of &lt;a href="http://lucene.apache.org/nutch/"&gt;Nutch&lt;/a&gt;. It's used throughout Hadoop as the mechanism by which daemons talk to each other. For example, a &lt;code&gt;DataNode&lt;/code&gt; communicates with the &lt;code&gt;NameNode&lt;/code&gt;  using the RPC interface &lt;code&gt;DatanodeProtocol&lt;/code&gt;.&lt;br /&gt;&lt;br /&gt;Protocols are defined using Java interfaces whose arguments and return types are primitives, Strings, Writables, or arrays. These types can all be serialized using Hadoop's specialized serialization format, based on &lt;a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/io/Writable.html"&gt;Writable&lt;/a&gt;. Combined with the magic of &lt;a href="http://java.sun.com/j2se/1.3/docs/guide/reflection/proxy.html"&gt;Java dynamic proxies&lt;/a&gt;, we get a simple RPC mechanism which for the caller appears to be a Java interface.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;MapReduce and Writables&lt;/h3&gt;Hadoop uses Writables for another, quite different, purpose: as a serialization format for MapReduce programs. If you've ever written a Hadoop MapReduce program you will have used Writables for the key and value types. For example:&lt;br /&gt;&lt;pre&gt;&lt;code&gt;&lt;br /&gt;&lt;br /&gt;public class MapClass&lt;br /&gt;implements Mapper&amp;lt;LongWritable, Text, Text, IntWritable&amp;gt; {&lt;br /&gt;&lt;br /&gt;// ...&lt;br /&gt;&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;(&lt;code&gt;Text&lt;/code&gt; is just a Writable version of Java &lt;code&gt;String&lt;/code&gt;.)&lt;br /&gt;&lt;br /&gt;The primary benefit of using Writables is in their efficiency. Compared to &lt;a href="http://java.sun.com/j2se/1.4.2/docs/guide/serialization/index.html"&gt;Java serialization&lt;/a&gt;, which would have been an obvious alternative choice, they have a more compact representation. Writables don't store their type in the serialized representation, since at the point of deserialization it is known which type is expected. For the MapReduce code above, the input key is a &lt;code&gt;LongWritable&lt;/code&gt;, so an empty &lt;code&gt;LongWritable&lt;/code&gt; instance is asked to populate itself from the input data stream.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;More flexible MapReduce&lt;/h3&gt;There are downsides of having to use Writables for MapReduce types, however. For a newcomer to Hadoop it's another hurdle: something else to learn ("why can't I just use a String?"). More seriously, perhaps, is that it's hard to use different binary storage formats for MapReduce input and output. For example, Apache Thrift (see below) is an increasingly popular way of storing binary data. It's possible, but cumbersome and inefficient, to read or write Thrift data from MapReduce.&lt;br /&gt;&lt;br /&gt;From Hadoop 0.17.0 onwards &lt;a href="https://issues.apache.org/jira/browse/HADOOP-1986"&gt;you no longer have to use Writables for key and value types in MapReduce programs&lt;/a&gt;. You can use any serialization framework. (Note that this is change is completely independent of Hadoop's RPC mechanism, which still uses Writables - and can only use Writables - as its on-wire format.) So it's easier to use Thrift types, say, throughout your MapReduce program. Or you can even use Java serialization (with &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3566"&gt;some limitations&lt;/a&gt; which will be fixed). What's more, you can add your own serialization framework if you like.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Record I/O, Thrift and Protocol Buffers&lt;br /&gt;&lt;/h3&gt;Another problem with Writables, at least for the MapReduce programmer, is that creating new types is a burden. You have to implement the Writable interface, which means designing the on-wire format, and writing two methods: one to write the data in that format and one to read it back.&lt;br /&gt;&lt;br /&gt;Hadoop's Record I/O was created to solve this problem. You write a definition of your types using a record definition language, then run a record compiler to generate Java source code representations of your types. All Record I/O types are Writable, so they plug into Hadoop very easily. As a bonus, you can generate bindings for other languages, so it's easy to read your data files from other programs.&lt;br /&gt;&lt;br /&gt;For whatever reason, Record I/O never really took off. It's used in &lt;a href="http://zookeeper.sourceforge.net/"&gt;ZooKeeper&lt;/a&gt;, but that's about it (and ZooKeeper will &lt;a href="http://publists.facebook.com/pipermail/thrift/2008-January/000330.html"&gt;move away&lt;/a&gt; from it someday). Momentum has switched to &lt;a href="http://incubator.apache.org/thrift/"&gt;Thrift&lt;/a&gt; (from Facebook, now in the Apache Incubator), which offers a very similar proposition, but in more languages. Thrift also makes it easy to build a (cross-language) RPC mechanism.&lt;br /&gt;&lt;br /&gt;Yesterday, Google open sourced &lt;a href="http://code.google.com/apis/protocolbuffers/"&gt;Protocol Buffers&lt;/a&gt;, its "language-neutral, platform-neutral, extensible mechanism for serializing structured data". Record I/O, Thrift and Protocol Buffers are really solving the same problem, so it will be interesting to see how this develops. Of course, since we're talking about persistent data formats, nothing's going to go away in the short or medium term while people have significant amounts of data locked up in these formats.&lt;br /&gt;&lt;br /&gt;That's why it makes sense to add support in Hadoop for MapReduce using Thrift and Protocol Buffers: so people can process data in the format they have it in. This will be a relatively simple addition.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;What Next?&lt;/h3&gt;For RPC, where a message is short-lived, changing the mechanism is more viable in the short term. Going back to Hadoop's RPC mechanism, now that both Thrift and Protocol Buffers offer an alternative, it may well be time to evaluate them to see if either can offer a performance boost. It would be a big job to retrofit RPC in Hadoop with another implementation, but if there are significant performance gains to be had, then it would be worth doing.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/329808993" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/2466668333674859777/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=2466668333674859777" title="3 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2466668333674859777?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2466668333674859777?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/07/rpc-and-serialization-with-hadoop.html" title="RPC and Serialization with Hadoop, Thrift, and Protocol Buffers" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-5017354658620266704</id><published>2008-07-03T14:09:00.005+01:00</published><updated>2008-07-03T14:33:27.405+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-07-03T14:33:27.405+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop beats terabyte sort record</title><content type="html">Hadoop has beaten the record for the terabyte sort benchmark, bringing it from 297 seconds to 209. Owen O'Malley wrote the MapReduce program (which by the way has a clever partitioner to ensure the reducer outputs are globally sorted and not just sorted per output partition, which is what the default sort does), and then ran it on 910 nodes on Yahoo!'s cluster. There are more details in Owen's &lt;a href="http://developer.yahoo.com/blogs/hadoop/2008/07/apache_hadoop_wins_terabyte_sort_benchmark.html"&gt;blog post&lt;/a&gt; (and there's a link to the benchmark page which has a PDF explaining his program). You can also look at the &lt;a href="http://svn.apache.org/viewvc/hadoop/core/trunk/src/examples/org/apache/hadoop/examples/terasort/"&gt;code in trunk&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Well done Owen and well done Hadoop!&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/325796106" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/5017354658620266704/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=5017354658620266704" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5017354658620266704?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5017354658620266704?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/07/hadoop-beats-terabyte-sort-record.html" title="Hadoop beats terabyte sort record" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7956405302605780166</id><published>2008-06-20T16:01:00.001+01:00</published><updated>2008-06-20T16:01:44.897+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-06-20T16:01:44.897+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Query Languages" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop Query Languages</title><content type="html">If you want a high-level query language for drilling into your huge Hadoop dataset, then you've got some choice:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://incubator.apache.org/pig/"&gt;Pig&lt;/a&gt;, from Yahoo! and now incubating at Apache, has an imperative language called Pig Latin for performing operations on large data files.&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.jaql.org/"&gt;Jaql&lt;/a&gt;, from IBM and soon to be open sourced, is a declarative query language for JSON data.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Hive, from Facebook and soon to &lt;a href="https://issues.apache.org/jira/browse/HADOOP-3601"&gt;become a Hadoop contrib module&lt;/a&gt;, is a data warehouse system with a declarative query language that is a hybrid of SQL and Hadoop streaming.&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;All three projects have different strengths, but there is plenty of scope for collaboration and cross-pollination, particularly in the query language. For example, at the &lt;a href="http://research.yahoo.com/node/2104"&gt;Hadoop Summit&lt;/a&gt; in March, Joydeep Sen Sarma of Facebook said that they would be receptive to users who wanted to use Pig Latin or Jaql in Hive. And Kevin Beyer of IBM Research said that Pig and Jaql are converging, and they've had discussions with the Pig team about how to bring them even closer together.&lt;br /&gt;&lt;br /&gt;Meanwhile, to learn more I recommend &lt;a href="http://www.cs.cmu.edu/%7Eolston/publications/sigmod08.pdf"&gt;Pig Latin: A Not-So-Foreign Language for Data Processing&lt;/a&gt; (by Chris Olston &lt;span style="font-style: italic;"&gt;et al&lt;/span&gt;&lt;span&gt;), and the &lt;a href="http://research.yahoo.com/node/2104"&gt;slides and videos from the Hadoop Summit&lt;/a&gt;.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;(And I haven't even included &lt;a href="http://www.cascading.org/"&gt;Cascading&lt;/a&gt;, from &lt;a href="http://chris.wensel.net/"&gt;Chris K. Wensel&lt;/a&gt;, which, while not a query language &lt;span style="font-style: italic;"&gt;per se&lt;/span&gt;, is an abstraction built on MapReduce for building data processing flows in Java or Groovy using a plumbing metaphor with constructs such as taps, pipes, and flows. Well worth a look too.)&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/316265539" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7956405302605780166/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7956405302605780166" title="4 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7956405302605780166?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7956405302605780166?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/06/hadoop-query-languages.html" title="Hadoop Query Languages" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">4</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-3339094449895398610</id><published>2008-06-13T13:15:00.000+01:00</published><updated>2008-06-13T13:16:48.105+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-06-13T13:16:48.105+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><category scheme="http://www.blogger.com/atom/ns#" term="Cloud Computing" /><title type="text">"The Next Big Thing"</title><content type="html">&lt;a href="http://mvdirona.com/jrh/perspectives/2007/10/29/WelcomeToPerspectives.aspx"&gt;James Hamilton&lt;/a&gt; on &lt;a href="http://perspectives.mvdirona.com/2008/01/15/TheNextBigThing.aspx"&gt;The Next Big Thing&lt;/a&gt;:&lt;blockquote&gt;Storing blobs in the sky is fine but pretty reproducible by any competitor.  Storing structured data as well as blobs is considerably more interesting but what has even more lasting business value is the storing data in the cloud AND providing a programming platform for multi-thousand node data analysis.  Almost every reasonable business on the planet has a complex set of dimensions that need to be optimized.&lt;/blockquote&gt;&lt;br /&gt;I think we're only beginning to see interesting data processing being done in the cloud - there's much more to come.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/311121048" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/3339094449895398610/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=3339094449895398610" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/3339094449895398610?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/3339094449895398610?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/05/next-big-thing.html" title="&quot;The Next Big Thing&quot;" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-5153878883282354355</id><published>2008-05-30T17:15:00.004+01:00</published><updated>2008-05-30T18:25:59.248+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-05-30T18:25:59.248+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Family" /><category scheme="http://www.blogger.com/atom/ns#" term="Mobile" /><title type="text">Bluetooth Castle</title><content type="html">&lt;a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="http://4.bp.blogspot.com/_IhqEHw4_Ick/SEAyN-dm-EI/AAAAAAAAAEw/o8lOSEi9SRM/s1600-h/raglan_castle.jpg"&gt;&lt;img style="margin: 0pt 10px 10px 0pt; float: left; cursor: pointer;" src="http://4.bp.blogspot.com/_IhqEHw4_Ick/SEAyN-dm-EI/AAAAAAAAAEw/o8lOSEi9SRM/s200/raglan_castle.jpg" alt="" id="BLOGGER_PHOTO_ID_5206216384927168578" border="0" /&gt;&lt;/a&gt;Today I visited &lt;a href="http://en.wikipedia.org/wiki/Raglan_Castle"&gt;Raglan Castle&lt;/a&gt; in Monmouthshire with my family. &lt;a href="http://www.cadw.wales.gov.uk/"&gt;Cadw&lt;/a&gt;, the government body that manages the castle, were running a trial to deliver audio files to visitors' mobile phones using Bluetooth. As I walked through the entrance I simply made my phone discoverable, waited a few seconds for the MP3 to download, then started listening to a &lt;a href="http://www.cadw-feedback.org.uk/audio_en.html"&gt;guided tour&lt;/a&gt; of the castle. It's a great use of the technology: it just worked.&lt;br /&gt;&lt;br /&gt;The talk only lasted a few minutes, so we had plenty of time afterwards to run around the ruins.&lt;br /&gt;&lt;br /&gt;A couple of technical questions that sprang to mind:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;How would you set up a server to push files over Bluetooth? (There are loads of ways you could use this - maps of the local area at transport hubs, sharing the schedule at conferences, random photo of the day at home, etc.)&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Can you make audio files navigable? That is, make it easy to go to the part of audio file that is about a given exhibit by typing in the exhibit's number? (This problem reminds me of Cliff Schmidt's &lt;a href="http://www.eu.apachecon.com/eu2008/program/talk/2625.html"&gt;talk&lt;/a&gt; about the Talking Book Device at ApacheCon EU 2008.)&lt;br /&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/301395904" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/5153878883282354355/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=5153878883282354355" title="3 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5153878883282354355?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5153878883282354355?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/05/bluetooth-castle.html" title="Bluetooth Castle" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://4.bp.blogspot.com/_IhqEHw4_Ick/SEAyN-dm-EI/AAAAAAAAAEw/o8lOSEi9SRM/s72-c/raglan_castle.jpg" height="72" width="72" /><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">3</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-4309697238982889737</id><published>2008-04-22T21:05:00.012+01:00</published><updated>2008-04-30T22:06:09.273+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-04-30T22:06:09.273+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><category scheme="http://www.blogger.com/atom/ns#" term="Cloud Computing" /><title type="text">Portable Cloud Computing</title><content type="html">Last July I asked "&lt;a href="http://www.lexemetech.com/2007/07/why-are-there-no-amazon-s3ec2.html"&gt;Why are there no Amazon S3/EC2 competitors?&lt;/a&gt;", lamenting the lack of competition in the utility or cloud computing market and the implications for disaster recovery. Closely tied to disaster recover is &lt;span style="font-style: italic;"&gt;portability&lt;/span&gt; -- the ability to switch between different utility computing providers as easily as I switch electricity suppliers. (OK, that can be a pain, at least in the UK, but it doesn't require rewiring my house.)&lt;br /&gt;&lt;br /&gt;It's useful to compare &lt;a href="http://aws.amazon.com/"&gt;Amazon Web Services&lt;/a&gt; with Google's recently launched &lt;a href="http://code.google.com/appengine/"&gt;App Engine&lt;/a&gt; in these terms. In some sense they compete, but they are strikingly different offerings. Amazon's services are &lt;span style="font-style: italic;"&gt;bottom up&lt;/span&gt;: "here's a CPU, now install your own software". Google's is &lt;span style="font-style: italic;"&gt;top down&lt;/span&gt;: "write some code to these APIs and we'll scale it for you". There's no way I can take my EC2 services and run them on App Engine. But I can do the reverse -- sort of -- thanks to &lt;a href="http://appdrop.com/"&gt;AppDrop.&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;And that's the thing. What I would love is a utility service from different providers that I can switch between. That's one of the meanings of "utility", after all. (Moving lots of data between providers may make this difficult or expensive to do in practice -- "data inertia" -- but that's not a reason not to have the capability.)&lt;br /&gt;&lt;br /&gt;There are at least two ways this can happen. One's the AppDrop approach -- emulate Google's API and provide an alternative place to run applications, in this case it's EC2.&lt;br /&gt;&lt;br /&gt;However, there's another way: build "standard, non-proprietary cloud APIs with open-source implementations", as Doug Cutting says on his blog post &lt;a href="http://blog.lucene.com/2008/04/09/cloud-commodity-or-proprietary/"&gt;Cloud: commodity or proprietary?&lt;/a&gt; In this world, applications use a common API, and host with whichever provider they fancy. Bottom up offerings like Amazon's facilitate this approach: the underlying platforms may differ, but it's easy to run your application on the provided platform -- for example, by building an Amazon AMI. Google's top down approach is not so amenable, application developers are locked-in to the APIs Google provide. (Of course, Google may open this platform up more over time, but it remains to be seen if they will open it up to the extent of being able to run arbitrary executables.)&lt;br /&gt;&lt;br /&gt;As Doug notes, &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt; is providing a lot of the building blocks for building cloud services: filesystem (&lt;a href="http://hadoop.apache.org/core/"&gt;HDFS&lt;/a&gt;), database (&lt;a href="http://hadoop.apache.org/hbase/"&gt;HBase&lt;/a&gt;), computation (&lt;a href="http://hadoop.apache.org/core/"&gt;MapReduce&lt;/a&gt;), coordination (&lt;a href="http://zookeeper.sourceforge.net/"&gt;ZooKeeper&lt;/a&gt;). And here, perhaps, is where the two approaches may meet -- AppDrop could be backed by HBase (or indeed &lt;a href="http://www.hypertable.org/"&gt;Hypertable&lt;/a&gt;), or HBase (and Hypertable) could have an open API which your application could use directly.&lt;br /&gt;&lt;br /&gt;Rails or Django on HBase, anyone?&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/281045834" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/4309697238982889737/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=4309697238982889737" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/4309697238982889737?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/4309697238982889737?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/04/portable-cloud-computing.html" title="Portable Cloud Computing" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-5714730535162695628</id><published>2008-04-14T20:43:00.005+01:00</published><updated>2008-04-14T21:34:52.212+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-04-14T21:34:52.212+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Conferences" /><category scheme="http://www.blogger.com/atom/ns#" term="Apache" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop at ApacheCon Europe</title><content type="html">On Friday in Amsterdam there was a lot of Hadoop on the menu at ApacheCon. I kicked it off at 9am with &lt;a href="http://eu.apachecon.com/eu2008/program/talk/2403"&gt;A Tour of Apache Hadoop&lt;/a&gt;, Owen O'Malley followed with &lt;a href="http://eu.apachecon.com/eu2008/program/talk/2524"&gt;Programming with Hadoop’s Map/Reduce&lt;/a&gt;, and &lt;span class="speaker_name"&gt;Allen Wittenauer&lt;/span&gt; finished off after lunch with &lt;a href="http://eu.apachecon.com/eu2008/program/talk/2535"&gt;Deploying Grid Services using Apache Hadoop&lt;/a&gt;. Find the slides on the &lt;a href="http://wiki.apache.org/hadoop/HadoopPresentations"&gt;Hadoop Presentations&lt;/a&gt; page of the wiki. I've also embedded mine below.&lt;br /&gt;&lt;br /&gt;I only saw half of Allen's talk as I had to catch my plane, but I was there long enough to see his interesting choice of HDFS users... :)&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div style="width: 425px; text-align: left;" id="__ss_352863"&gt;&lt;object style="margin: 0px;" height="355" width="425"&gt;&lt;param name="movie" value="http://static.slideshare.net/swf/ssplayer2.swf?doc=apacheconeu2008hadooptourtomwhite-1208199764129275-9"&gt;&lt;param name="allowFullScreen" value="true"&gt;&lt;param name="allowScriptAccess" value="always"&gt;&lt;embed src="http://static.slideshare.net/swf/ssplayer2.swf?doc=apacheconeu2008hadooptourtomwhite-1208199764129275-9" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" height="355" width="425"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;div style="font-size: 11px; font-family: tahoma,arial; height: 26px; padding-top: 2px;"&gt;&lt;a href="http://www.slideshare.net/?src=embed"&gt;&lt;img src="http://static.slideshare.net/swf/logo_embd.png" style="border: 0px none ; margin-bottom: -5px;" alt="SlideShare" /&gt;&lt;/a&gt; | &lt;a href="http://www.slideshare.net/tomwhite/apache-con-eu2008-hadoop-tour-tom-white?src=embed" title="View 'Apache Con Eu2008 Hadoop Tour Tom White' on SlideShare"&gt;View&lt;/a&gt; | &lt;a href="http://www.slideshare.net/upload?src=embed"&gt;Upload your own&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;br /&gt;Also at ApacheCon I enjoyed meeting the &lt;a href="http://lucene.apache.org/mahout/"&gt;Mahout&lt;/a&gt; people (Grant, Karl, Isabel and Erik), seeing &lt;a href="http://eu.apachecon.com/eu2008/program/talk/2625"&gt;Cliff Schmidt's keynote&lt;/a&gt;, and generally meeting interesting people.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/270230404" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/5714730535162695628/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=5714730535162695628" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5714730535162695628?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/5714730535162695628?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/04/hadoop-at-apachecon-europe.html" title="Hadoop at ApacheCon Europe" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-8179153698514682004</id><published>2008-03-30T20:48:00.007+01:00</published><updated>2008-03-30T22:05:25.884+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-03-30T22:05:25.884+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Amazon EC2" /><title type="text">Turn off the lights when you're not using them, please</title><content type="html">One of the things that struck me about this week's &lt;a href="http://developer.amazonwebservices.com/connect/ann.jspa?annID=295"&gt;new Amazon EC2 features&lt;/a&gt; was the pricing model for Elastic IP addresses:&lt;br /&gt;&lt;blockquote&gt;$0.01 per hour when &lt;span style="font-weight: bold;"&gt;not&lt;/span&gt; mapped to a running instance&lt;br /&gt;&lt;/blockquote&gt;The idea is to encourage people to stop hogging public IP addresses, which are a limited resource, when they don't need them.&lt;br /&gt;&lt;br /&gt;I think one way of viewing EC2 - and the other Amazon utility services - is as a way of putting very fine-grained costs on various computing operations.  So will such a pricing model drive us to minimise the computing resources we use to solve a particular problem? My hope is that making computing costs more transparent will at least make us think about what we're using more, in the way metered electricity makes (some of) us think twice about leaving the lights on. Perhaps we'll even start talking about &lt;a href="http://weblogs.java.net/blog/tomwhite/archive/2006/08/s3map.html"&gt;optimizing for monetary cost&lt;/a&gt; or energy usage rather than purely raw speed?&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/260894961" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/8179153698514682004/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=8179153698514682004" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/8179153698514682004?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/8179153698514682004?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/03/turn-off-lights-when-youre-not-using.html" title="Turn off the lights when you're not using them, please" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7735453237479750871</id><published>2008-03-23T18:49:00.005Z</published><updated>2008-03-23T21:53:36.702Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-03-23T21:53:36.702Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Visualization" /><category scheme="http://www.blogger.com/atom/ns#" term="Easter" /><title type="text">Visualizing Easter</title><content type="html">I made this image a few years ago (as a postcard to give to friends), but it's appropriate to show again today as it's a neat visual demonstration that Easter this year is the earliest this century.&lt;br /&gt;&lt;br /&gt;&lt;a href="http://flickr.com/photos/8797345@N02/2355962134/"&gt;&lt;img src="http://farm3.static.flickr.com/2288/2355962134_b29b261631_d.jpg" width="349" height="500" alt="101 Easters"/&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;The scale at the bottom shows the maximum range of Easter: from 22 March to 25 April. You can read more about the image &lt;a href="http://tiling.org/longview/index.html"&gt;here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The image is licensed under a &lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/"&gt;Creative Commons Attribution-Noncommercial-Share Alike license&lt;/a&gt;, so you are free to share and remix it for non-commercial purposes.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/256687681" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7735453237479750871/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7735453237479750871" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7735453237479750871?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7735453237479750871?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/03/visualizing-easter.html" title="Visualizing Easter" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7837841027525509394</id><published>2008-03-22T09:44:00.004Z</published><updated>2008-04-30T15:03:00.311+01:00</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-04-30T15:03:00.311+01:00</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Learning MapReduce</title><content type="html">&lt;span style="font-style: italic;"&gt;&lt;span style="font-weight: bold;"&gt;Update&lt;/span&gt;: I've posted &lt;/span&gt;&lt;a style="font-style: italic;" href="http://wiki.apache.org/hadoop-data/attachments/HadoopPresentations/attachments/MapReduce-SPA2008-answers.pdf"&gt;my answers&lt;/a&gt;&lt;span style="font-style: italic;"&gt; to the exercises. Let me know if you find any mistakes. &lt;span style="font-weight: bold;"&gt;Also&lt;/span&gt;: Tamara Petroff has posted a &lt;a href="http://rfa.blog.idnet.com/?p=14"&gt;write up&lt;/a&gt; of the session.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;On Wednesday [19 March], I ran a session at &lt;a href="http://www.spaconference.org/spa2008/"&gt;SPA 2008&lt;/a&gt; entitled "&lt;a href="http://www.spaconference.org/spa2008/sessions/session153.html"&gt;Understanding MapReduce with Hadoop&lt;/a&gt;". SPA is a very hands-on conference, with many sessions having a methodological slant, so I wanted to get people who had never encountered MapReduce before actually writing MapReduce programs. I only had 75 minutes, so I decided against getting people coding on their laptops. (In hindsight this was a good decision, as I went to several other sessions where we struggled to get the software installed.) Instead, we wrote MapReduce programs on paper, using a simplified notation.&lt;br /&gt;&lt;br /&gt;It seemed to work. For the first half hour, I gave as minimal an introduction to MapReduce as I could, then the whole group spent the next half hour working in pairs to express the solutions to a number of exercises as MapReduce programs. We spent the last 15 minutes comparing notes and discussing some of the solutions to the problems.&lt;br /&gt;&lt;br /&gt;There were six exercises, presented in rough order of difficulty, and I'm pleased to say that every pair managed to solve at least one. Here's some of the feedback I got:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Some struggled to know what input data formats to use. Perhaps I glossed over this too much - I didn't want people to worry about precisely how the data was encoded - but I could have emphasised more that you can have the data presented to your map function in any way that's convenient.&lt;/li&gt;&lt;li&gt;While most people understood the notation I used for writing the map and reduce functions, it did cause some confusion. For example, someone wanted to see the example code again so they could understand what was going on. And another person said it took a while to realise that they could do arbitrary processing as a part of the map and reduce functions. It would be interesting to do the session again but using Java notation.&lt;/li&gt;&lt;li&gt;It was quite common for people to try to do complex things in their map and reduce functions - they felt bad if they just used an identity function, because it was somehow a waste. And on a related note, chaining map reduce jobs together wasn't obvious to many. But once pointed out, folks had an "aha!" moment and were quick to exploit it.&lt;/li&gt;&lt;li&gt;The fact that you typically get multiple reduce outputs prompted questions from some - "but how do you combine them into a single answer?". Talking about chained MapReduce helped here again.&lt;/li&gt;&lt;li&gt;Everyone agreed that it wasn't much like functional programming.&lt;/li&gt;&lt;/ul&gt;You can find the &lt;a href="http://wiki.apache.org/hadoop-data/attachments/HadoopPresentations/attachments/MapReduce-SPA2008.pdf"&gt;slides&lt;/a&gt; on the Hadoop wiki. They include the six exercises, which I've reproduced below, in rough order of difficulty. (I'll post my answers next week.)&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Find the [number of] hits by 5 minute timeslot for a website given its access logs.&lt;/li&gt;&lt;li&gt;Find the pages with over 1 million hits in day for a website given its access logs.&lt;/li&gt;&lt;li&gt;Find the pages that link to each page in a collection of webpages.&lt;/li&gt;&lt;li&gt;Calculate the proportion of lines that match a given regular expression for a collection of documents.&lt;/li&gt;&lt;li&gt;Sort tabular data by a primary and secondary column.&lt;/li&gt;&lt;li&gt;Find the most popular pages for a website given its access logs.&lt;/li&gt;&lt;/ol&gt;Is this a good list of exercises? Do you have any exercises that you've found useful for learning MapReduce?&lt;br /&gt;&lt;br /&gt;Finally, thanks to &lt;a href="http://chatley.com/"&gt;Robert Chatley&lt;/a&gt; for being a guinea pig for the exercises, and for helping out on the day with participants' questions during the session.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/255988203" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7837841027525509394/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7837841027525509394" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7837841027525509394?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7837841027525509394?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/03/learning-mapreduce.html" title="Learning MapReduce" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-2860680421013796857</id><published>2008-03-18T13:04:00.000Z</published><updated>2008-03-18T13:04:42.500Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-03-18T13:04:42.500Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Hardware" /><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">"Disks have become tapes"</title><content type="html">MapReduce is a programming model for processing vast amounts of data. One of the reasons that it works so well is because it exploits a sweet spot of modern disk drive technology trends. In essence MapReduce works by repeatedly sorting and merging data that is streamed to and from disk at the &lt;span style="font-style: italic;"&gt;transfer rate&lt;/span&gt; of the disk. Contrast this to accessing data from a relational database that operates at the &lt;span style="font-style: italic;"&gt;seek rate&lt;/span&gt; of the disk (seeking is the process of moving the disk's head to a particular place on the disk to read or write data).&lt;br /&gt;&lt;br /&gt;So why is this interesting? Well, look at the trends in seek time and transfer rate. Seek time has grown at about 5% a year, whereas transfer rate at about 20% &lt;a href="http://www.blogger.com/post-edit.g?blogID=8898949683610477251&amp;amp;postID=2860680421013796857#1"&gt;[1]&lt;/a&gt;. Seek time is growing more slowly than transfer rate - &lt;span style="font-style: italic;"&gt;so it pays to use a model that operates at the transfer rate&lt;/span&gt;. Which is what MapReduce does. I first saw this observation in Doug Cutting's talk, with Eric Baldeschwieler, at &lt;a href="http://conferences.oreillynet.com/os2007/"&gt;OSCON&lt;/a&gt; last year, where he worked through the numbers for updating a 1 terabyte database using the two paradigms B-Tree (seek-limited) and Sort/Merge (transfer-limited). (See the &lt;a href="http://wiki.apache.org/hadoop-data/attachments/HadoopPresentations/attachments/oscon-part-1.pdf"&gt;slides&lt;/a&gt; and &lt;a href="http://us.dl1.yimg.com/download.yahoo.com/dl/ydn/hadoop.m4v"&gt;video&lt;/a&gt; for more detail.)&lt;br /&gt;&lt;br /&gt;The general point was well summed up by Jim Gray in an &lt;a href="http://www.acmqueue.org/modules.php?name=Content&amp;amp;pa=showpage&amp;amp;pid=43"&gt;interview&lt;/a&gt; in ACM Queue from 2003:&lt;br /&gt;&lt;blockquote&gt;... programmers have to start thinking of the disk as a sequential device rather than a random access device.&lt;/blockquote&gt;Or the more pithy: "Disks have become tapes." (Quoted by &lt;a href="http://www.databasecolumn.com/2007/09/disk-trends.html"&gt;David DeWitt&lt;/a&gt;.)&lt;br /&gt;&lt;br /&gt;But even the growth of transfer rate is dwarfed by another measure of disk drives - capacity, which is growing at about 50% a year. David DeWitt &lt;a href="http://www.databasecolumn.com/2007/09/disk-trends.html"&gt;argues&lt;/a&gt; that since the effective transfer rate of drives is falling we need database systems that work with this trend - such as column-store databases and wider use of compression (since this effectively increases the transfer rate of a disk). Of existing databases he says:&lt;br /&gt;&lt;blockquote&gt;Already we see transaction processing systems running on farms of mostly empty disk drives to obtain enough seeks/second to satisfy their transaction processing rates.&lt;/blockquote&gt;But this applies to transfer rate too (or if it doesn't yet, it will). Replace "seeks" with "transfers" and "transaction processing" with "MapReduce" and I think over time we'll start seeing Hadoop installations that choose to use large numbers of smaller capacity disks to maximize their processing rates.&lt;br /&gt;&lt;br /&gt;&lt;span style="font-size:85%;"&gt;&lt;a name="1"&gt;[1]&lt;/a&gt; See &lt;a href="http://www.cs.utexas.edu/users/dahlin/techTrends/trends.disk.ps"&gt;Trends in Disk Technology&lt;/a&gt; by Michael D. Dahlin for changes between 1987-1994. For the period since then these figures still hold - as it's relatively easy to check using manufacturer's data sheets, although with seek time it's harder to tell since the definitions seem to change from year to year and from manufacturer to manufacturer. Still, 5% is generous.&lt;/span&gt;&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/253636543" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/2860680421013796857/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=2860680421013796857" title="17 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2860680421013796857?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2860680421013796857?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/03/disks-have-become-tapes.html" title="&quot;Disks have become tapes&quot;" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">17</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7201201271658646958</id><published>2008-03-02T01:03:00.002Z</published><updated>2008-03-02T01:29:15.603Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-03-02T01:29:15.603Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">MapReduce without the Reduce</title><content type="html">There's a class of MapReduce applications that use Hadoop just for its distributed processing capabilities. Telltale signs are:&lt;br /&gt;&lt;br /&gt;1. Little or no input data of note. (Certainly not large files stored in HDFS.)&lt;br /&gt;2. Map tasks are therefore not limited by their ability to consume input, but by their ability to run the task, which depending on the application may be CPU-bound or IO-bound.&lt;br /&gt;3. Little or map output.&lt;br /&gt;4. No reducers (set by &lt;code&gt;conf.setNumReduceTasks(0)&lt;/code&gt;).&lt;br /&gt;&lt;br /&gt;This seems to work well - indeed the &lt;a href="http://hadoop.apache.org/core/docs/current/api/index.html"&gt;CopyFiles&lt;/a&gt; program in Hadoop (aka &lt;code&gt;distcp&lt;/code&gt;) follows this pattern to efficiently copy files between distributed filesystems:&lt;br /&gt;&lt;br /&gt;1. The input to each map task is a source file and a destination.&lt;br /&gt;2. The map task is limited by its ability to copy the source to the destination (IO-bound).&lt;br /&gt;3. The map output is used as a convenience to record files that were skipped.&lt;br /&gt;4. There are no reducers.&lt;br /&gt;&lt;br /&gt;Combined with &lt;a href="http://hadoop.apache.org/core/docs/current/streaming.html"&gt;Streaming&lt;/a&gt; this is a neat way to distribute your processing in any language. You do need a Hadoop cluster, it is true, but CPU-intensive jobs would happily co-exist with more traditional MapReduce jobs, which are typically fairly light on CPU usage.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/244103494" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7201201271658646958/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7201201271658646958" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7201201271658646958?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7201201271658646958?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/03/mapreduce-without-reduce.html" title="MapReduce without the Reduce" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-8111742251020614276</id><published>2008-02-12T20:28:00.000Z</published><updated>2008-02-12T21:03:59.190Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-02-12T21:03:59.190Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Amazon EC2" /><title type="text">Forgotten EC2 instances</title><content type="html">I noticed today that I had an EC2 development cluster running that I hadn't shut down from a few days ago. It was only a couple of instances, but even so, it was annoying. &lt;a href="http://www.1060.org/blogxter/publish/5"&gt;Steve Loughran&lt;/a&gt; had a good idea for preventing this: have the cluster shut itself down if it detects you go offline - by using your chat presence. You'd probably want to build a bit of a delay into it to avoid losing work due to some network turbulence, but it would work nicely for short lived clusters which are brought up simply to do a bit of number crunching. Alternatively, and perhaps more lo-tech, the cluster could just email you every few hours to say "I'm still here!".&lt;br /&gt;&lt;br /&gt;I wonder how many forgotten instances are running at Amazon at any one time. Is there a mass calling of &lt;span style="font-family:courier new;"&gt;ec2-terminate-instances&lt;/span&gt; every month end when the owners see their bills?&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/233978125" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/8111742251020614276/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=8111742251020614276" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/8111742251020614276?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/8111742251020614276?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/02/forgotten-ec2-instances.html" title="Forgotten EC2 instances" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-4902324713504908641</id><published>2008-02-01T13:30:00.000Z</published><updated>2008-02-01T14:32:33.533Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-02-01T14:32:33.533Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Thrift" /><category scheme="http://www.blogger.com/atom/ns#" term="HBase" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Apache Incubator Proposal for Thrift</title><content type="html">There's a &lt;a href="http://wiki.apache.org/incubator/ThriftProposal"&gt;proposal&lt;/a&gt; for &lt;a href="http://developers.facebook.com/thrift/"&gt;Thrift&lt;/a&gt; to go into the &lt;a href="http://incubator.apache.org/"&gt;Apache Incubator&lt;/a&gt;. This seems to me to be a good move - there's increasing interest in Thrift - just look at the number of language bindings that have been contributed: Cocoa/Objective C, C++, C#, Erlang, Haskell, Java, OCaml, Perl, PHP, Python, Ruby, and Squeak at the last count. It's even &lt;a href="http://blog.leetsoft.com/2007/12/30/compiling-thrift-on-osx"&gt;fairly painless&lt;/a&gt; to compile on Mac OS X now, although it'd be nice to have a Java version of the compiler.&lt;br /&gt;&lt;br /&gt;Also, there are some nice synergies with other Apache projects - it is already being used in HBase, and there are moves to make it easier to use in Hadoop Core as a &lt;a href="https://issues.apache.org/jira/browse/HADOOP-1986"&gt;serialization format&lt;/a&gt; (so MapReduce jobs can consume and produce Thrift-formatted data).&lt;br /&gt;&lt;br /&gt;If the proposal is accepted it will be interesting to see what happens to Hadoop's own language-neutral record serialization package, &lt;a href="http://hadoop.apache.org/core/docs/current/api/org/apache/hadoop/record/package-summary.html"&gt;Record I/O&lt;/a&gt;. The momentum is certainly with Thrift and discussions on the &lt;a href="http://lists.pub.facebook.com/mailman/listinfo/thrift"&gt;mailing list&lt;/a&gt; suggest that stuff will eventually be &lt;a href="http://lists.pub.facebook.com/pipermail/thrift/2008-January/000330.html"&gt;ported&lt;/a&gt; to use Thrift.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/227286784" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/4902324713504908641/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=4902324713504908641" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/4902324713504908641?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/4902324713504908641?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/02/apache-incubator-proposal-for-thrift.html" title="Apache Incubator Proposal for Thrift" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7098839651234570081</id><published>2008-01-30T11:38:00.000Z</published><updated>2008-01-30T12:25:47.499Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-01-30T12:25:47.499Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop and Log File Analysis</title><content type="html">I've always thought that Hadoop is a great fit for analyzing log files (I even wrote &lt;a href="http://developer.amazonwebservices.com/connect/entry.jspa?externalID=873"&gt;an article&lt;/a&gt; about it). The big win is that you can write &lt;span style="font-style: italic;"&gt;ad hoc&lt;/span&gt; MapReduce queries against huge datasets and get results in minutes or hours. So I was interested to read Stu Hood's recent &lt;a href="http://blog.racklabs.com/wp-trackback.php?p=66"&gt;post&lt;/a&gt; about using Hadoop to analyze email log data:&lt;br /&gt;&lt;blockquote&gt;Here at &lt;a href="http://www.mailtrust.com/"&gt;Mailtrust&lt;/a&gt;, Rackspace’s mail division, we are taking advantage of Hadoop to help us wrangle several hundred gigabytes of email log data that our mail servers generate each day. We’ve built a great tool for our support team that lets them search mail logs in order to troubleshoot problems for customers. Until recently, this log search and storage system was centered around a traditional relational database, which worked fine until the exponential growth in the volume of our dataset overcame what a single machine could cope with. The new logging backend we’ve developed based on Hadoop gives us virtually unlimited scalability.&lt;/blockquote&gt;The best bit was when they wrote a MapReduce query to find the geographic distribution of their users.&lt;br /&gt;&lt;blockquote&gt;This data was so useful that we’ve scheduled the MapReduce job to run monthly and we will be using this data to help us decide which Rackspace data centers to place new mail servers in as we grow.&lt;/blockquote&gt;It's great when a technology has the ability to make such a positive contribution to your business. In Doug Cutting's &lt;a href="http://www.25hoursaday.com/weblog/CommentView.aspx?guid=b8df6e9b-b8f1-4159-a9bf-617cdcf5f60e#515ad298-4067-4645-b4db-b4709b89ad6e"&gt;words&lt;/a&gt;, it is "transformative".&lt;br /&gt;&lt;br /&gt;Can we take this further? It seems to me that there is a gap in the market for an open source web traffic analysis tool. Think Google Analytics where you can write your own queries. I wonder who's going to build such a thing?&lt;br /&gt;&lt;blockquote&gt;&lt;/blockquote&gt;&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/225875153" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7098839651234570081/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7098839651234570081" title="1 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7098839651234570081?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7098839651234570081?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/01/hadoop-and-log-file-analysis.html" title="Hadoop and Log File Analysis" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">1</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7050164914307978061</id><published>2008-01-16T21:51:00.000Z</published><updated>2008-01-16T21:53:10.134Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-01-16T21:53:10.134Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Hadoop is now an Apache Top Level Project</title><content type="html">Doug Cutting just reported on the Hadoop lists that the Apache board voted this morning (US time) to make Hadoop a TLP. Until now it has been a Lucene subproject, which made sense when Hadoop was broken out from the Nutch codebase two years ago. Since then Hadoop has grown dramatically. This change will make it possible for a number of associated projects - such as &lt;a href="http://wiki.apache.org/lucene-hadoop/Hbase"&gt;HBase&lt;/a&gt; and &lt;a href="http://incubator.apache.org/pig/"&gt;Pig&lt;/a&gt; - to become Hadoop subprojects in their own right. (There are more details in the &lt;a href="http://www.nabble.com/Hadoop-TLP--td14326977.html"&gt;original discussion&lt;/a&gt;.)&lt;br /&gt;&lt;br /&gt;Thanks for looking after us Lucene - it's been a great time so far and we'll keep in touch!&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/224176020" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7050164914307978061/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7050164914307978061" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7050164914307978061?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7050164914307978061?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/01/hadoop-is-now-apache-top-level-project.html" title="Hadoop is now an Apache Top Level Project" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-2092018556482689634</id><published>2008-01-13T22:05:00.000Z</published><updated>2008-01-13T22:09:21.361Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-01-13T22:09:21.361Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="HBase" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">MapReduce, Map Reduce, Map/Reduce or Map-Reduce?</title><content type="html">Although I've seen the other variants (and used some of them myself), Google call it &lt;a href="http://labs.google.com/papers/mapreduce.html"&gt;"MapReduce"&lt;/a&gt;, so that seems like the right thing to call it to me, since they invented it. The usage figures seem to back up this conclusion. "MapReduce" (no space) has 87,000 Google hits, while "Map Reduce" (with space) has only 50,200, and the latter includes "Map/Reduce" and "Map-Reduce" variants, since Google (and search engines in general) ignore punctuation.&lt;br /&gt;&lt;br /&gt;In this age of case sensitivity and camel case one has to watch out for these things. In fact, I've only just realised that the Hadoop database is called "HBase", not "Hbase". The curse of &lt;a href="http://wiki.apache.org/lucene-hadoop/Hbase"&gt;wiki names&lt;/a&gt;. And the logo doesn't help either - it's all caps!&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/224176021" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/2092018556482689634/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=2092018556482689634" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2092018556482689634?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/2092018556482689634?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/01/mapreduce-map-reduce-mapreduce-or-map.html" title="MapReduce, Map Reduce, Map/Reduce or Map-Reduce?" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-9088902805788058972</id><published>2008-01-07T09:47:00.000Z</published><updated>2008-01-07T10:06:18.419Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2008-01-07T10:06:18.419Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="MapReduce" /><category scheme="http://www.blogger.com/atom/ns#" term="Hadoop" /><title type="text">Casual Large Scale Data Processing</title><content type="html">I think Greg Linden hits the nail on the head when he says &lt;a href="http://glinden.blogspot.com/2008/01/mapreducing-20-petabytes-per-day.html"&gt;of MapReduce at Google&lt;/a&gt;:&lt;br /&gt;&lt;blockquote&gt;What is so remarkable about this is how casual it makes large scale data processing. Anyone at Google can write a MapReduce program that uses hundreds or thousands of machines from their cluster. Anyone at Google can process terabytes of data. And they can get their results back in about 10 minutes, so they can iterate on it and try something else if they didn't get what they wanted the first time.&lt;/blockquote&gt;I think this is a good way of looking at what Hadoop is trying to achieve - to make large scale data processing as natural as small scale data processing. Hadoop can provide infrastructure to do this, but there is also a need for open datasets that are MapReducable. Imagine if I could run a MapReduce program against Google's web crawl database, Amazon's sales data or Facebook's social graph.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/224176022" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/9088902805788058972/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=9088902805788058972" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/9088902805788058972?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/9088902805788058972?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2008/01/casual-large-scale-data-processing.html" title="Casual Large Scale Data Processing" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8898949683610477251.post-7962883833546939815</id><published>2007-11-27T21:43:00.000Z</published><updated>2007-11-27T22:10:08.739Z</updated><app:edited xmlns:app="http://purl.org/atom/app#">2007-11-27T22:10:08.739Z</app:edited><category scheme="http://www.blogger.com/atom/ns#" term="Music" /><category scheme="http://www.blogger.com/atom/ns#" term="Quantum Mechanics" /><title type="text">Eels in Many Worlds</title><content type="html">I caught the second half of &lt;a href="http://www.bandweblogs.com/blog/2007/11/16/parallel-worlds-parallel-lives-bbc-four-documentary-about-eels-founder-mark-everett-and-his-father/"&gt;Parallel Worlds, Parallel Lives&lt;/a&gt; on BBC Four last night, where Mark Everett of the band &lt;a href="http://en.wikipedia.org/wiki/Eels_%28band%29"&gt;Eels&lt;/a&gt; travelled around the US to find out about his father Hugh Everett III, the physicist who developed the &lt;a href="http://en.wikipedia.org/wiki/Many-worlds_interpretation"&gt;Many Worlds Interpretation&lt;/a&gt; of quantum mechanics. (New Scientist has a good &lt;a href="http://www.newscientist.com/channel/opinion/mg19626311.800-interview-parallel-lives-can-never-touch.html"&gt;interview&lt;/a&gt; with him too.)&lt;br /&gt;&lt;br /&gt;I read Everett senior's &lt;a href="http://www.univer.omsk.su/omsk/Sci/Everett/paper1957.html"&gt;landmark paper&lt;/a&gt; when I was doing my Master's degree, and at the time I agreed with Paul Davies' view that Many Worlds is "cheap on assumptions but expensive on universes". I later read David Deutsch's &lt;a href="http://www.qubit.org/people/david/FabricOfReality/FoR.html"&gt;The Fabric of Reality&lt;/a&gt;, and I was prepared to give the theory more time - but even so, I still find it a lot to ask. (It's better than the &lt;a href="http://en.wikipedia.org/wiki/Copenhagen_interpretation"&gt;Copenhagen Interpretation&lt;/a&gt; though.) Not long after finishing my degree I got into Eels and saw them play live in London.&lt;br /&gt;&lt;br /&gt;Only now do I know the connection. A fascinating programme.&lt;img src="http://feeds.feedburner.com/~r/TomWhite/~4/224176023" height="1" width="1"/&gt;</content><link rel="replies" type="application/atom+xml" href="http://www.lexemetech.com/feeds/7962883833546939815/comments/default" title="Post Comments" /><link rel="replies" type="text/html" href="https://www.blogger.com/comment.g?blogID=8898949683610477251&amp;postID=7962883833546939815" title="0 Comments" /><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7962883833546939815?v=2" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8898949683610477251/posts/default/7962883833546939815?v=2" /><link rel="alternate" type="text/html" href="http://www.lexemetech.com/2007/11/eels-in-many-worlds.html" title="Eels in Many Worlds" /><author><name>Tom White</name><uri>http://www.blogger.com/profile/02418758537880869494</uri><email>noreply@blogger.com</email></author><thr:total xmlns:thr="http://purl.org/syndication/thread/1.0">0</thr:total></entry></feed>
